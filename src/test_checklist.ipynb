{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "homeless-perry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:28:44.689017Z",
     "start_time": "2021-05-18T16:28:42.590088Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import RNNCellBase, Parameter\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "convinced-dairy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:28:44.711286Z",
     "start_time": "2021-05-18T16:28:44.690402Z"
    },
    "code_folding": [
     0,
     17,
     37
    ]
   },
   "outputs": [],
   "source": [
    "class MaskedDataSet(torch.utils.data.Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, partial_recipe, goal, ingredients, masked):\n",
    "        self.X = partial_recipe\n",
    "        self.y = masked\n",
    "        self.goal = goal\n",
    "        self.ingredients = ingredients\n",
    "\n",
    "        assert len(self.X) == len(self.y), print(\"Number of examples don't match up\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index], self.goal[index], self.ingredients[index]\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, wv_matrix):\n",
    "        super(Model, self).__init__()\n",
    "        vocab_size, embedding_size = wv_matrix.shape\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(wv_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        hidden_size = embedding_size\n",
    "        self.cgru = CustomChecklistCell(embedding_size, hidden_size)\n",
    "        self.fc = torch.nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, recipe, g, ingr):\n",
    "        recipe_embed = self.embedding(recipe)\n",
    "        goal_embed = self.embedding(g)\n",
    "        ingr_embed = self.embedding(ingr)\n",
    "        goal_embed = goal_embed.sum(axis=1)\n",
    "        output, ht, a, E_t_new = self.cgru(recipe_embed, goal_embed, ingr_embed)\n",
    "        logits = self.fc(output)\n",
    "        return logits, ht, a, E_t_new\n",
    "\n",
    "class CustomChecklistCell(RNNCellBase):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True, batch_first=True, beta=5, gamma=2):\n",
    "        super(CustomChecklistCell, self).__init__(input_size, hidden_size, bias, num_chunks=5)\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.Z = Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.Y = Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.U_g = Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "\n",
    "        self.z_bias = Parameter(torch.ones(hidden_size))\n",
    "        self.y_bias = Parameter(torch.ones(hidden_size))\n",
    "\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.S = Parameter(torch.ones(3, hidden_size))\n",
    "        self.P = Parameter(torch.ones(hidden_size, hidden_size))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def init_hidden(self, g):\n",
    "        return F.linear(g, self.U_g)\n",
    "\n",
    "    def cell(self, inp, hidden, g, E_t_new, activation=F.tanh):\n",
    "\n",
    "        w_ih = self.weight_ih\n",
    "        w_hh = self.weight_hh\n",
    "        b_ih = self.bias_ih\n",
    "        b_hh = self.bias_hh\n",
    "        Y = self.Y\n",
    "        Z = self.Z\n",
    "        y_bias = self.y_bias\n",
    "        z_bias = self.z_bias\n",
    "\n",
    "        # Calculate all W @ inp. w_ih is actually 5 different weights\n",
    "        gi = F.linear(inp, w_ih, b_ih)\n",
    "\n",
    "        # Calcualtes all W @ hidden, w_hh is actually 5 different weights\n",
    "        gh = F.linear(hidden, w_hh, b_hh)\n",
    "\n",
    "        # Split them into seperate terms\n",
    "        i_reset, i_update, i_new, i_goal, i_item = gi.chunk(5, 1)\n",
    "        h_reset, h_update, h_new, h_goal, h_item = gh.chunk(5, 1)\n",
    "\n",
    "        # Update Gate\n",
    "        z_t = F.sigmoid(i_update + h_update)\n",
    "\n",
    "        # Reset Gate\n",
    "        r_t = F.sigmoid(i_reset + h_reset)\n",
    "\n",
    "        # Goal Select Gate\n",
    "        s_t = F.sigmoid(i_goal + h_goal)\n",
    "\n",
    "        # Item Select Gate\n",
    "        q_t = F.sigmoid(i_item + h_item)\n",
    "\n",
    "        # That term\n",
    "        tmp = torch.einsum('mlk -> mk', E_t_new)\n",
    "\n",
    "        # New Gate\n",
    "        h_tilde_t = activation(i_new\n",
    "                               + r_t * h_new\n",
    "                               + s_t * F.linear(g, Y, y_bias)\n",
    "                               + q_t * F.linear(tmp, Z, z_bias))\n",
    "\n",
    "        # tp1: t plus 1\n",
    "        h_tp1 = h_tilde_t + z_t * (hidden - h_tilde_t)\n",
    "\n",
    "        return h_tp1\n",
    "\n",
    "    def attention(self, ht, a, E):\n",
    "        ref_type = torch.nn.Softmax()(F.linear(self.beta * ht, self.S))\n",
    "        h_proj = F.linear(ht, self.P)\n",
    "\n",
    "        E_new = torch.einsum('ml, mlk -> mlk', 1 - a, E)\n",
    "        alpha_new = self.gamma * torch.einsum('mk,mlk->ml', h_proj, E_new)\n",
    "        alpha_new = torch.nn.Softmax()(alpha_new)\n",
    "        c_new = torch.einsum('mlk, ml -> mk', E, alpha_new)\n",
    "\n",
    "        E_used = torch.einsum('ml, mlk -> mlk', a, E)\n",
    "        alpha_used = self.gamma * torch.einsum('mk,mlk->ml', h_proj, E_used)\n",
    "        alpha_used = torch.nn.Softmax()(alpha_used)\n",
    "        c_used = torch.einsum('mlk, ml -> mk', E, alpha_used)\n",
    "\n",
    "        c_gru = h_proj\n",
    "\n",
    "        out = ref_type[:, 0].reshape(-1, 1)*c_gru + ref_type[:, 1].reshape(-1, 1)*c_new + ref_type[:, 2].reshape(-1, 1)*c_used\n",
    "        a = a + ref_type[:, 1].reshape(-1, 1) * alpha_new\n",
    "\n",
    "        return out, a, E_new\n",
    "\n",
    "    def forward(self, inp, g, E):\n",
    "\n",
    "        # E will need padding as well so that it can be minibatched\n",
    "        '''\n",
    "        inp: (examples, seq_length, inp_dim) assuming batch_first\n",
    "        g: (examples, hidden_dim)\n",
    "        E: (examples, agenda length, hidden_dim)\n",
    "        '''\n",
    "        L = E.shape[1]\n",
    "\n",
    "        ht = self.init_hidden(g)\n",
    "        a = torch.zeros(inp.shape[0], L).to(device)\n",
    "\n",
    "        if len(inp.shape) == 3:\n",
    "            if self.batch_first:\n",
    "                inp = inp.transpose(0, 1)\n",
    "\n",
    "        E_t_new = torch.einsum('ml, mlk -> mlk', 1-a, E)\n",
    "\n",
    "        lst_o = list()\n",
    "        ot = torch.zeros_like(inp[0])\n",
    "        zero = torch.zeros_like(inp[0])\n",
    "        for t in np.arange(inp.shape[0]):\n",
    "\n",
    "            if not torch.allclose(inp[t], zero):\n",
    "                ht, ot, a, E_t_new = self.step(inp[t], g, E, ht, a, E_t_new)\n",
    "\n",
    "            lst_o.append(ot)\n",
    "\n",
    "        output = torch.stack(lst_o)\n",
    "\n",
    "        if self.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "\n",
    "        return output, ht, a, E_t_new\n",
    "\n",
    "    def step(self, inp, g, E, ht=None, a=None, E_t_new=None):\n",
    "\n",
    "        if ht is None:\n",
    "            ht = self.init_hidden(g)\n",
    "\n",
    "        if a is None:\n",
    "            L = E.shape[1]\n",
    "            a = torch.zeros(inp.shape[0], L).to(device)\n",
    "            E_t_new = torch.einsum('ml, mlk -> mlk', 1 - a, E)\n",
    "\n",
    "        ht = self.cell(\n",
    "            inp, ht, g, E_t_new\n",
    "        )\n",
    "\n",
    "        ot, a, E_t_new = self.attention(ht, a, E)\n",
    "\n",
    "        return ht, ot, a, E_t_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worst-hardwood",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:28:44.718122Z",
     "start_time": "2021-05-18T16:28:44.714460Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, word2idx, sent_type='goal'):\n",
    "    unk_index = word2idx['UNK']\n",
    "    sos_index = word2idx['SOS']\n",
    "    eos_index = word2idx['EOS']\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    list_of_tokens = []\n",
    "    if sent_type == 'recipe':\n",
    "        list_of_tokens.append(sos_index)\n",
    "        \n",
    "    sentence = sentence.split(' ') if type(sentence) is type('x') else sentence\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in word2idx:\n",
    "            list_of_tokens.append(word2idx[word])\n",
    "        else:\n",
    "            list_of_tokens.append(unk_index)\n",
    "    \n",
    "    if sent_type == 'recipe':\n",
    "        list_of_tokens.append(eos_index)\n",
    "    \n",
    "    return list_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acoustic-advancement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:28:44.723186Z",
     "start_time": "2021-05-18T16:28:44.719444Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def tokens_to_sent(tokens, append=True):\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        if token != 0:\n",
    "            try:\n",
    "                words.append(idx2word[token])\n",
    "            except:\n",
    "                words.append(idx2word[token.item()])\n",
    "                \n",
    "    return ' '.join(words) if append else words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "committed-particle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:28:44.729658Z",
     "start_time": "2021-05-18T16:28:44.724407Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def generate_text(g, E, method='greedy'):\n",
    "\n",
    "    # EXPERIMENTAL!\n",
    "    '''\n",
    "    inputs: \n",
    "    g (1, seq_len)\n",
    "    E (1, num_of_ingredients)\n",
    "    method: random or greedy\n",
    "    '''\n",
    "\n",
    "    tokens = []\n",
    "    words = []\n",
    "\n",
    "    a = None\n",
    "    E_t_new = None\n",
    "    ht = None\n",
    "\n",
    "    g = model.embedding(g)\n",
    "    E = model.embedding(E)\n",
    "    g = g.sum(axis=1)\n",
    "\n",
    "    token = torch.tensor([word2idx['SOS']]).to(device)\n",
    "    tokens.append(token)\n",
    "\n",
    "    for i in range(1000):\n",
    "        inp = model.embedding(tokens[i].squeeze())\n",
    "        inp = inp[None, :]\n",
    "\n",
    "        ht, ot, a, E_t_new = model.cgru.step(inp, g, E, ht, a, E_t_new)\n",
    "        logits = model.fc(ot)\n",
    "        \n",
    "        \n",
    "        #print(logits.shape)\n",
    "        \n",
    "        if method == 'greedy':\n",
    "            out = model.fc(ot)\n",
    "            token = torch.argmax(out)\n",
    "        elif method == 'random':\n",
    "            dist = torch.distributions.categorical.Categorical(logits=logits[0])\n",
    "            token = dist.sample()\n",
    "        \n",
    "        tokens.append(token)\n",
    "        words.append(idx2word[token.item()])\n",
    "        \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "trained-merit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:28:44.740622Z",
     "start_time": "2021-05-18T16:28:44.730761Z"
    },
    "code_folding": [
     0,
     22,
     83
    ]
   },
   "outputs": [],
   "source": [
    "def beam_search_with_start_token(g, E, N=3, start_token='SOS', ht=None, a=None, E_t_new=None, MAX_LEN = 50):\n",
    "    tokens = []\n",
    "    words = []\n",
    "\n",
    "    g_emb = model.embedding(g)\n",
    "    E_emb = model.embedding(E)\n",
    "    g_emb = g_emb.sum(axis=1)\n",
    "    \n",
    "    # First dct creation\n",
    "    \n",
    "    token = torch.tensor([word2idx[start_token]]).to(device)\n",
    "    tokens.append(token)\n",
    "\n",
    "    inp = model.embedding(tokens[0].squeeze())\n",
    "    inp = inp[None, :]\n",
    "    ht, ot, a, E_t_new = model.cgru.step(inp, g_emb, E_emb, ht, a, E_t_new)\n",
    "    logits = model.fc(ot)\n",
    "    log_softmax = torch.nn.LogSoftmax()\n",
    "    probs = log_softmax(logits)\n",
    "    values, indices = torch.topk(probs, N)\n",
    "\n",
    "    dct = dict()\n",
    "    for n in range(N):\n",
    "        dct[f'seq{n}'] = dict()\n",
    "        dct[f'seq{n}']['tokens'] = tokens + [indices.squeeze()[n]]\n",
    "        dct[f'seq{n}']['log_prob'] = values.squeeze().tolist()[n]\n",
    "        dct[f'seq{n}']['prev'] = (ht, a, E_t_new)\n",
    "        \n",
    "    del tokens, words\n",
    "\n",
    "    # Actual Beam Search\n",
    "    vocab_size = len(probs.squeeze())\n",
    "\n",
    "    for j in range(MAX_LEN - 1):\n",
    "\n",
    "        lst_probs = []\n",
    "        lst_prevs = []\n",
    "\n",
    "        for n in range(N):\n",
    "            tokens = dct[f'seq{n}']['tokens']\n",
    "            ht, a, E_t_new =  dct[f'seq{n}']['prev']\n",
    "            prev_prob = dct[f'seq{n}']['log_prob']\n",
    "\n",
    "\n",
    "            inp = model.embedding(tokens[-1].squeeze())\n",
    "            inp = inp[None, :]\n",
    "\n",
    "            ht, ot, a, E_t_new = model.cgru.step(inp, g_emb, E_emb, ht, a, E_t_new)\n",
    "            logits = model.fc(ot)\n",
    "            probs = log_softmax(logits)\n",
    "\n",
    "            lst_probs.append(probs + prev_prob)\n",
    "            lst_prevs.append((ht, a, E_t_new))\n",
    "\n",
    "        values, indices = torch.stack(lst_probs).squeeze().reshape(-1).topk(N)\n",
    "\n",
    "        idxs = indices % vocab_size\n",
    "        seqs = indices // vocab_size\n",
    "        \n",
    "        new_dct = {}\n",
    "        for i, n in enumerate(seqs):\n",
    "            tokens = dct[f'seq{n}']['tokens'] + [idxs[i]]\n",
    "            ht, a, E_t_new =  lst_prevs[n]\n",
    "            current_prob = values[i].item()\n",
    "\n",
    "            new_dct[f'seq{i}'] = dict()\n",
    "            new_dct[f'seq{i}']['tokens'] = tokens\n",
    "            new_dct[f'seq{i}']['log_prob'] = current_prob\n",
    "            new_dct[f'seq{i}']['prev'] = (ht, a, E_t_new)\n",
    "            \n",
    "\n",
    "        dct = new_dct\n",
    "        \n",
    "        if dct[f'seq0']['tokens'][-1].item() == 10343:\n",
    "            break\n",
    "\n",
    "        assert len(dct) == N\n",
    "        \n",
    "    dct['seq0']['tokens'] = [w.item() for w in dct['seq0']['tokens']]\n",
    "        \n",
    "    return dct['seq0']['tokens']\n",
    "\n",
    "\n",
    "def mask_sentence(tokens, mask=0.5):\n",
    "    idx = int((1 - mask) * len(tokens))\n",
    "    inp = tokens[:idx]\n",
    "    masked = tokens[idx:]\n",
    "    return inp, masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "occasional-cowboy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:28:46.632680Z",
     "start_time": "2021-05-18T16:28:44.742186Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "emb_mat = np.load('emb_mat.npy')\n",
    "\n",
    "a_file = open(\"word2idx.json\", \"r\")\n",
    "word2idx = json.load(a_file)\n",
    "a_file.close()\n",
    "\n",
    "idx2word = {v:k for k,v in word2idx.items()}\n",
    "\n",
    "model = Model(emb_mat).to(device)\n",
    "model.load_state_dict(torch.load('classifier_new_10.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lined-queue",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:28:48.674998Z",
     "start_time": "2021-05-18T16:28:46.633845Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('train_fixed.pkl')\n",
    "df_test = pd.read_pickle('test_fixed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "related-student",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:28:50.716792Z",
     "start_time": "2021-05-18T16:28:48.676202Z"
    },
    "code_folding": [
     0,
     4
    ]
   },
   "outputs": [],
   "source": [
    "df_train[['0.5_inp', '0.5_out']] = pd.DataFrame(df_train['tokenized_instructions']\\\n",
    "                                       .apply(lambda x: mask_sentence(x, mask=0.5))\\\n",
    "                                       .tolist(), index=df_train.index)\n",
    "\n",
    "df_train[['0.2_inp', '0.8_out']] = pd.DataFrame(df_train['tokenized_instructions']\\\n",
    "                                       .apply(lambda x: mask_sentence(x, mask=0.8))\\\n",
    "                                       .tolist(), index=df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "recreational-mambo",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:28:50.852265Z",
     "start_time": "2021-05-18T16:28:50.717985Z"
    },
    "code_folding": [
     0,
     4
    ]
   },
   "outputs": [],
   "source": [
    "df_test[['0.5_inp', '0.5_out']] = pd.DataFrame(df_test['tokenized_instructions']\\\n",
    "                                       .apply(lambda x: mask_sentence(x, mask=0.5))\\\n",
    "                                       .tolist(), index=df_test.index)\n",
    "\n",
    "df_test[['0.2_inp', '0.8_out']] = pd.DataFrame(df_test['tokenized_instructions']\\\n",
    "                                       .apply(lambda x: mask_sentence(x, mask=0.8))\\\n",
    "                                       .tolist(), index=df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hourly-illustration",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:28:51.511850Z",
     "start_time": "2021-05-18T16:28:50.853395Z"
    }
   },
   "outputs": [],
   "source": [
    "goal_test = pad_sequences(df_test['tokenized_goal'])\n",
    "ingr_test = pad_sequences(df_test['tokenized_ingredients'])\n",
    "\n",
    "recipe_half_test = pad_sequences(df_test['0.5_inp'])\n",
    "recipe_fifth_test = pad_sequences(df_test['0.2_inp'])\n",
    "\n",
    "label_half_test = pad_sequences(df_test['0.5_out'])\n",
    "label_fifth_test = pad_sequences(df_test['0.8_out'])\n",
    "\n",
    "\n",
    "dataloader_params = {'batch_size': 1, 'shuffle': True, 'num_workers': 6}\n",
    "\n",
    "test_half_data = MaskedDataSet(recipe_half_test, goal_test, ingr_test, label_half_test)\n",
    "test_half_generator = torch.utils.data.DataLoader(test_half_data, **dataloader_params)\n",
    "\n",
    "test_fifth_data = MaskedDataSet(recipe_fifth_test, goal_test, ingr_test, label_fifth_test)\n",
    "test_fifth_generator = torch.utils.data.DataLoader(test_fifth_data, **dataloader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caroline-zimbabwe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:28:51.524985Z",
     "start_time": "2021-05-18T16:28:51.513140Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def beam_search(g, E, N=3, start_sent='SOS', start_tokens=None, MAX_LEN = 200):\n",
    "    tokens = []\n",
    "    words = []\n",
    "\n",
    "    g_emb = model.embedding(g)\n",
    "    E_emb = model.embedding(E)\n",
    "    g_emb = g_emb.sum(axis=1)\n",
    "    \n",
    "    # First dct creation\n",
    "    \n",
    "    if start_tokens is not None:\n",
    "        ot, ht, a, E_t_new = model.forward(start_tokens, g, E)\n",
    "        # Get logits for next word\n",
    "        logits = ot[:, -1, :]\n",
    "    elif start_sent is not None:\n",
    "        inp = model.embedding(torch.tensor(tokenize_sentence(start_sent, word2idx)).to(device))\n",
    "        ht, ot, a, E_t_new = model.cgru.step(inp, g_emb, E_emb)\n",
    "        # Get logits for next word\n",
    "        logits = model.fc(ot)\n",
    "    else:\n",
    "        raise Exception('Please provide tokens or a sentence as input!')\n",
    "        \n",
    "    log_softmax = torch.nn.LogSoftmax()\n",
    "    probs = log_softmax(logits)\n",
    "    values, indices = torch.topk(probs, N)\n",
    "\n",
    "    dct = dict()\n",
    "    for n in range(N):\n",
    "        dct[f'seq{n}'] = dict()\n",
    "        dct[f'seq{n}']['tokens'] = tokens + [indices.squeeze()[n]]\n",
    "        dct[f'seq{n}']['log_prob'] = values.squeeze().tolist()[n]\n",
    "        dct[f'seq{n}']['prev'] = (ht, a, E_t_new)\n",
    "        \n",
    "    del tokens, words\n",
    "\n",
    "    # Actual Beam Search\n",
    "    vocab_size = len(probs.squeeze())\n",
    "\n",
    "    \n",
    "    #print(tokens)\n",
    "    for j in range(MAX_LEN - 1):\n",
    "\n",
    "        lst_probs = []\n",
    "        lst_prevs = []\n",
    "\n",
    "        for n in range(N):\n",
    "            tokens = dct[f'seq{n}']['tokens']\n",
    "            ht, a, E_t_new =  dct[f'seq{n}']['prev']\n",
    "            prev_prob = dct[f'seq{n}']['log_prob']\n",
    "\n",
    "            \n",
    "            inp = model.embedding(tokens[-1].squeeze())\n",
    "            inp = inp[None, :]\n",
    "\n",
    "\n",
    "            ht, ot, a, E_t_new = model.cgru.step(inp, g_emb, E_emb, ht, a, E_t_new)\n",
    "            logits = model.fc(ot)\n",
    "            probs = log_softmax(logits)\n",
    "\n",
    "            lst_probs.append(probs + prev_prob)\n",
    "            lst_prevs.append((ht, a, E_t_new))\n",
    "\n",
    "        values, indices = torch.stack(lst_probs).squeeze().reshape(-1).topk(N)\n",
    "\n",
    "        idxs = indices % vocab_size\n",
    "        seqs = indices // vocab_size\n",
    "        \n",
    "        new_dct = {}\n",
    "        for i, n in enumerate(seqs):\n",
    "            tokens = dct[f'seq{n}']['tokens'] + [idxs[i]]\n",
    "            ht, a, E_t_new =  lst_prevs[n]\n",
    "            current_prob = values[i].item()\n",
    "\n",
    "            new_dct[f'seq{i}'] = dict()\n",
    "            new_dct[f'seq{i}']['tokens'] = tokens\n",
    "            new_dct[f'seq{i}']['log_prob'] = current_prob\n",
    "            new_dct[f'seq{i}']['prev'] = (ht, a, E_t_new)\n",
    "            \n",
    "\n",
    "        dct = new_dct\n",
    "        \n",
    "        if dct[f'seq0']['tokens'][-1].item() == 10343:\n",
    "            break\n",
    "\n",
    "        assert len(dct) == N\n",
    "        \n",
    "    dct['seq0']['tokens'] = [w.item() for w in dct['seq0']['tokens']]\n",
    "        \n",
    "    return dct['seq0']['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alpine-manchester",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T16:28:51.531404Z",
     "start_time": "2021-05-18T16:28:51.526198Z"
    }
   },
   "outputs": [],
   "source": [
    "# dct = {\n",
    "#     'generated_text': [],\n",
    "#     'goal': [],\n",
    "#     'ingredients': [],\n",
    "#     'label': []\n",
    "# }\n",
    "\n",
    "# c = 0\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for data in test_half_generator:\n",
    "#         recipe, label, goal, ingr = data\n",
    "#         recipe, label = recipe.type(torch.LongTensor).to(device), label.type(torch.LongTensor).to(device)\n",
    "#         goal, ingr = goal.type(torch.LongTensor).to(device), ingr.type(torch.LongTensor).to(device)\n",
    "\n",
    "#         pred = beam_search(goal, ingr, start_tokens=recipe)\n",
    "\n",
    "#         dct['generated_text'].append(tokens_to_sent(pred).split(' '))\n",
    "#         dct['goal'].append(tokens_to_sent(goal.squeeze()))\n",
    "#         dct['ingredients'].append(tokens_to_sent(ingr.squeeze(), append=False))\n",
    "#         dct['label'].append(tokens_to_sent(label.squeeze()).split(' '))\n",
    "        \n",
    "#         c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "drawn-steal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T20:21:14.112501Z",
     "start_time": "2021-05-18T16:28:51.532681Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/sai/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-2-e2f33a68f50b>:111: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  ref_type = torch.nn.Softmax()(F.linear(self.beta * ht, self.S))\n",
      "<ipython-input-2-e2f33a68f50b>:116: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha_new = torch.nn.Softmax()(alpha_new)\n",
      "<ipython-input-2-e2f33a68f50b>:121: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  alpha_used = torch.nn.Softmax()(alpha_used)\n",
      "<ipython-input-12-b43072aa4d02>:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = log_softmax(logits)\n",
      "<ipython-input-12-b43072aa4d02>:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = log_softmax(logits)\n"
     ]
    }
   ],
   "source": [
    "dct = {\n",
    "    'generated_text': [],\n",
    "    'goal': [],\n",
    "    'ingredients': [],\n",
    "    'label': []\n",
    "}\n",
    "\n",
    "c = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_fifth_generator:\n",
    "        recipe, label, goal, ingr = data\n",
    "        recipe, label = recipe.type(torch.LongTensor).to(device), label.type(torch.LongTensor).to(device)\n",
    "        goal, ingr = goal.type(torch.LongTensor).to(device), ingr.type(torch.LongTensor).to(device)\n",
    "\n",
    "        pred = beam_search(goal, ingr, start_tokens=recipe)\n",
    "\n",
    "        dct['generated_text'].append(tokens_to_sent(pred).split(' '))\n",
    "        dct['goal'].append(tokens_to_sent(goal.squeeze()))\n",
    "        dct['ingredients'].append(tokens_to_sent(ingr.squeeze(), append=False))\n",
    "        dct['label'].append(tokens_to_sent(label.squeeze()).split(' '))\n",
    "        \n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "increasing-museum",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-18T21:32:39.707324Z",
     "start_time": "2021-05-18T21:32:38.581838Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('80_percent_masked.json', 'w') as f:\n",
    "    json.dump(dct, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-association",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
