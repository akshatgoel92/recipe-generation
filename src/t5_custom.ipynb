{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of t5_custom.ipynb","provenance":[{"file_id":"1e4xANQNLQc1d2JeshvDY_JUIllotoM_R","timestamp":1620327150064}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lfCfH_lnEXeZ","executionInfo":{"status":"ok","timestamp":1620327755772,"user_tz":-330,"elapsed":29904,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}},"outputId":"7860eb27-4261-4a67-8082-5cfe1ff15c1d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIeRMOg3EXee","executionInfo":{"status":"ok","timestamp":1620327755777,"user_tz":-330,"elapsed":29892,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}},"outputId":"6a4fbdfa-cd38-4ce9-fcb8-ff84c1dc51e9"},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Thu May  6 19:02:35 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KG5Fal3xEXef","executionInfo":{"status":"ok","timestamp":1620327774279,"user_tz":-330,"elapsed":48382,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}},"outputId":"6bda44df-b803-41b1-bb44-bd6bbd91661a"},"source":["# Install these packages with these specific versions else the notebook breaks\n","!pip install transformers\n","!pip install pytorch_lightning\n","!pip install sentencepiece"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 8.4MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 49.6MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 55.2MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n","Collecting pytorch_lightning\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/0c/e2d52147ac12a77ee4e7fd7deb4b5f334cfb335af9133a0f2780c8bb9a2c/pytorch_lightning-1.2.10-py3-none-any.whl (841kB)\n","\u001b[K     |████████████████████████████████| 849kB 7.7MB/s \n","\u001b[?25hCollecting fsspec[http]>=0.8.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n","\u001b[K     |████████████████████████████████| 112kB 25.6MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (20.9)\n","Collecting future>=0.17.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n","\u001b[K     |████████████████████████████████| 829kB 18.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.19.5)\n","Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.4.1)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.8.1+cu101)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.41.1)\n","Collecting PyYAML!=5.4.*,>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n","\u001b[K     |████████████████████████████████| 276kB 36.5MB/s \n","\u001b[?25hCollecting torchmetrics==0.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/42/d984612cabf005a265aa99c8d4ab2958e37b753aafb12f31c81df38751c8/torchmetrics-0.2.0-py3-none-any.whl (176kB)\n","\u001b[K     |████████████████████████████████| 184kB 36.5MB/s \n","\u001b[?25hCollecting aiohttp; extra == \"http\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 23.8MB/s \n","\u001b[?25hRequirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (2.23.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch_lightning) (2.4.7)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.15.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.8.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.32.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (56.0.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.28.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.12.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.0.1)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.36.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.12.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.3.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch_lightning) (3.7.4.3)\n","Collecting yarl<2.0,>=1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n","\u001b[K     |████████████████████████████████| 296kB 53.8MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n","\u001b[K     |████████████████████████████████| 143kB 52.1MB/s \n","\u001b[?25hRequirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.0.4)\n","Collecting async-timeout<4.0,>=3.0\n","  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (20.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (2.10)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.2.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.10.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.4.1)\n","Building wheels for collected packages: future, PyYAML\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=b2fab14208f7cd48f5821febdda18845017616e3b43bfb92c8decd3129d7b071\n","  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n","  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=4ea6afb221a3701346c41bd20ed4feaca553477fc2d48383678c865f5f36a1f5\n","  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n","Successfully built future PyYAML\n","Installing collected packages: multidict, yarl, async-timeout, aiohttp, fsspec, future, PyYAML, torchmetrics, pytorch-lightning\n","  Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.3.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.4.0 future-0.18.2 multidict-5.1.0 pytorch-lightning-1.2.10 torchmetrics-0.2.0 yarl-1.6.3\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 6.0MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FKKEhtH6EXeg","executionInfo":{"status":"ok","timestamp":1620327780417,"user_tz":-330,"elapsed":54496,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}},"outputId":"8bb9df06-857c-4081-b833-298e61764874"},"source":["# Import packages\n","import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","import re\n","from itertools import chain\n","from string import punctuation\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pytorch_lightning as pl\n","import sentencepiece\n","\n","\n","from transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup\n",")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ll0uvgpIEXei","executionInfo":{"status":"ok","timestamp":1620327780418,"user_tz":-330,"elapsed":54492,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}}},"source":["import csv\n","from dataclasses import dataclass\n","\n","from enum import Enum\n","from typing import List, Optional\n","from transformers import PreTrainedTokenizer"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"7IQo2COLEXej","executionInfo":{"status":"ok","timestamp":1620327780419,"user_tz":-330,"elapsed":54490,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}}},"source":["def set_seed(seed):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed(42)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"MgLpsMkqEXej","executionInfo":{"status":"ok","timestamp":1620327780419,"user_tz":-330,"elapsed":54481,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}}},"source":["logger = logging.getLogger(__name__)\n","\n","class LoggingCallback(pl.Callback):\n","  def on_validation_end(self, trainer, pl_module):\n","    logger.info(\"***** Validation results *****\")\n","    if pl_module.is_logger():\n","      metrics = trainer.callback_metrics\n","      # Log results\n","      for key in sorted(metrics):\n","        if key not in [\"log\", \"progress_bar\"]:\n","          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","\n","  def on_test_end(self, trainer, pl_module):\n","    logger.info(\"***** Test results *****\")\n","\n","    if pl_module.is_logger():\n","      metrics = trainer.callback_metrics\n","\n","      # Log and save results to file\n","      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n","      with open(output_test_results_file, \"w\") as writer:\n","        for key in sorted(metrics):\n","          if key not in [\"log\", \"progress_bar\"]:\n","            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZgrDdkhREXek","executionInfo":{"status":"ok","timestamp":1620327780420,"user_tz":-330,"elapsed":54477,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}}},"source":["class T5FineTuner(pl.LightningModule):\n","  def __init__(self, hparams):\n","    super(T5FineTuner, self).__init__()\n","    self.hparams = hparams\n","    \n","    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n","    self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n","  \n","  def is_logger(self):\n","    return self.trainer.global_rank <= 0\n","  \n","  def forward(\n","      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n","  ):\n","    return self.model(\n","        input_ids,\n","        attention_mask=attention_mask,\n","        decoder_input_ids=decoder_input_ids,\n","        decoder_attention_mask=decoder_attention_mask,\n","        labels=labels,\n","    )\n","\n","  def _step(self, batch):\n","    labels = batch[\"target_ids\"]\n","    labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","    outputs = self(\n","        input_ids=batch[\"source_ids\"],\n","        attention_mask=batch[\"source_mask\"],\n","        labels=labels,\n","        decoder_attention_mask=batch['target_mask']\n","    )\n","\n","    loss = outputs[0]\n","    \n","    return loss\n","\n","  def training_step(self, batch, batch_idx):\n","    loss = self._step(batch)\n","    self.log('training_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","    tensorboard_logs = {\"train_loss\": loss}\n","    return {\"loss\": loss, \"log\": tensorboard_logs}\n","  \n","  def training_epoch_end(self, outputs):\n","    avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n","    self.log('avg_training_loss', avg_train_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","    tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n","    return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","  def validation_step(self, batch, batch_idx):\n","    loss = self._step(batch)\n","    self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","    return {\"val_loss\": loss}\n","  \n","  def validation_epoch_end(self, outputs):\n","    avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n","    tensorboard_logs = {\"val_loss\": avg_loss}\n","    return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","  def configure_optimizers(self):\n","    \"Prepare optimizer and schedule (linear warmup and decay)\"\n","\n","    model = self.model\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": self.hparams.weight_decay,\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n","    self.opt = optimizer\n","    return [optimizer]\n","  \n","  def optimizer_step(self,\n","                     epoch=None, \n","                     batch_idx=None, \n","                     optimizer=None, \n","                     optimizer_idx=None, \n","                     optimizer_closure=None, \n","                     on_tpu=None, \n","                     using_native_amp=None, \n","                     using_lbfgs=None\n","                     ):\n","\n","    optimizer.step(closure=optimizer_closure)\n","    optimizer.zero_grad()\n","    self.lr_scheduler.step()\n","\n","  def train_dataloader(self):\n","    train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n","    dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n","    t_total = (\n","        (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n","        // self.hparams.gradient_accumulation_steps\n","        * float(self.hparams.num_train_epochs)\n","    )\n","    scheduler = get_linear_schedule_with_warmup(\n","        self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n","    )\n","    self.lr_scheduler = scheduler\n","    return dataloader\n","\n","  def val_dataloader(self):\n","    val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n","    return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"UUR4zS45EXeo","executionInfo":{"status":"ok","timestamp":1620327780421,"user_tz":-330,"elapsed":54473,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}}},"source":["# The below code is adapted from:\n","# https://github.com/huggingface/transformers/blob/master/examples/multiple-choice/utils_multiple_choice.py\n","\n","@dataclass(frozen=True)\n","class InputExample:\n","    \"\"\"\n","    A single training/test example for multiple choice\n","    Args:\n","        example_id: Unique id for the example.\n","        question: string. The untokenized text of the second sequence (question).\n","        contexts: list of str. The untokenized text of the first sequence (context of corresponding question).\n","        endings: list of str. multiple choice's options. Its length must be equal to contexts' length.\n","        label: (Optional) string. The label of the example. This should be\n","        specified for train and dev examples, but not for test examples.\n","    \"\"\"\n","\n","    name: str\n","    ingredients: str\n","    steps: List[str]\n","    label: Optional[str]\n","\n","class Split(Enum):\n","    train = \"train\"\n","    dev = \"dev\"\n","    test = \"test\"\n","\n","class DataProcessor:\n","    \"\"\"Base class for data converters for multiple choice data sets.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_test_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_labels(self):\n","        \"\"\"Gets the list of labels for this data set.\"\"\"\n","        raise NotImplementedError()\n","\n","class RecipeProcessor(DataProcessor):\n","    \"\"\"Processor for the SWAG data set.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} train\".format(data_dir))\n","        return self._create_examples(self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\")\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n","        return self._create_examples(self._read_csv(os.path.join(data_dir, \"val.csv\")), \"dev\")\n","\n","    def get_test_examples(self, data_dir):\n","        \"\"\"See base class.\"\"\"\n","        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n","        raise ValueError(\n","            \"For swag testing, the input file does not contain a label column. It can not be tested in current code\"\n","            \"setting!\"\n","        )\n","        return self._create_examples(self._read_csv(os.path.join(data_dir, \"test.csv\")), \"test\")\n","\n","    def get_labels(self):\n","        \"\"\"See base class.\"\"\"\n","        return [\"0\", \"1\", \"2\", \"3\"]\n","\n","    def _read_csv(self, input_file):\n","        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n","            return list(csv.reader(f))\n","\n","    def _create_examples(self, lines: List[List[str]], type: str):\n","        \"\"\"Creates examples for the training and dev sets.\"\"\"\n","\n","        examples = [\n","            InputExample(\n","                name=line[1],\n","                # common beginning of each\n","                # choice is stored in \"sent2\".\n","                ingredients=eval(line[2]),\n","                steps=eval(line[3]),\n","                label=eval(line[3]),\n","            )\n","            for line in lines[1:]  # we skip the line with the column names\n","        ]\n","\n","        return examples"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vbcg-MsjEXer","executionInfo":{"status":"ok","timestamp":1620327780423,"user_tz":-330,"elapsed":54469,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}}},"source":["class RecipeDataset(Dataset):\n","    \n","    def __init__(self, tokenizer, data_dir, type_path,  max_len=512, mask_percent=0.4):\n","        self.data_dir = data_dir\n","        self.type_path = type_path\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer\n","        self.inputs = []\n","        self.targets = []\n","        self.mask_percent = mask_percent\n","        \n","        self.proc = RecipeProcessor()\n","        self._build()\n","  \n","    def __getitem__(self, index):\n","        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n","        target_ids = self.targets[index][\"input_ids\"].squeeze()\n","\n","        src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","\n","        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \n","                \"target_ids\": target_ids, \"target_mask\": target_mask}\n","  \n","    def __len__(self):\n","        return len(self.inputs)\n","  \n","    def _build(self):\n","        if self.type_path == 'train':\n","            examples = self.proc.get_train_examples(self.data_dir)\n","        else:\n","            examples = self.proc.get_dev_examples(self.data_dir)\n","    \n","        for i, example in enumerate(examples):\n","            if i % 10000 == 0:\n","                print(i)\n","            self._create_features(example)\n","  \n","    def _create_masks(self, steps):\n","        \n","        words = [word for step in steps for word in step.split(\" \")]\n","        \n","        total_words = len(words)\n","        if total_words == 0:\n","          mask_words = 0\n","        elif total_words < 100:\n","          mask_words = np.round(self.mask_percent*total_words).astype(int)\n","        else:\n","          mask_words = 50\n","        mask_indices = np.random.choice(np.arange(total_words), mask_words)\n","        \n","        input_words = []\n","        label = []\n","        j = 0\n","        \n","        for i, word in enumerate(words):\n","            if i in mask_indices:\n","                # Add a sentinel token in place of the word to be masked\n","                input_words.append('<extra_id_{}>'.format(j))\n","                # Add this word's token to the label list\n","                label.append('<extra_id_{}>'.format(j))\n","                # Add the corresponding label to the list\n","                label.append(word)\n","                # Step the counter by 1\n","                j+=1\n","            # If this index is not among the sampled indices \n","            # just append the word\n","            else:\n","                input_words.append(word)\n","         \n","        # Add in an extra ID token like the format says\n","        label.append('<extra_id_{}>'.format(j))\n","\n","        label = \" \".join(label)\n","        input_words = \" \".join(input_words)\n","        \n","        return(input_words, label)\n","    \n","    def _create_features(self, example):\n","    \n","        ingredients_ = \",\".join(example.ingredients)\n","        name_ = example.name\n","        \n","        masked_, label_ = self._create_masks(example.steps) \n","                                        \n","        input_ = \"name: %s  ingredients: %s masked: %s </s>\" % (name_, ingredients_, masked_)\n","        target =  label_ + \" </s>\"\n","        \n","        # tokenize inputs\n","        tokenized_inputs = self.tokenizer.batch_encode_plus(\n","            [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n","        )\n","        # tokenize targets\n","        tokenized_targets = self.tokenizer.batch_encode_plus(\n","            [target], max_length=60, pad_to_max_length=True, return_tensors=\"pt\"\n","        )\n","\n","        self.inputs.append(tokenized_inputs)\n","        self.targets.append(tokenized_targets)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"-dKj3KplGmJZ"},"source":["class RecipeGenDataset(Dataset):\n","    \n","    def __init__(self, tokenizer, data_dir, type_path,  max_len=512, mask_percent=0.4):\n","        self.data_dir = data_dir\n","        self.type_path = type_path\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer\n","        self.inputs = []\n","        self.targets = []\n","        self.mask_percent = mask_percent\n","        \n","        self.proc = RecipeProcessor()\n","        self._build()\n","  \n","    def __getitem__(self, index):\n","        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n","        target_ids = self.targets[index][\"input_ids\"].squeeze()\n","\n","        src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","\n","        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \n","                \"target_ids\": target_ids, \"target_mask\": target_mask}\n","  \n","    def __len__(self):\n","        return len(self.inputs)\n","  \n","    def _build(self):\n","        if self.type_path == 'train':\n","            examples = self.proc.get_train_examples(self.data_dir)\n","        else:\n","            examples = self.proc.get_dev_examples(self.data_dir)\n","    \n","        for i, example in enumerate(examples):\n","            if i % 10000 == 0:\n","                print(i)\n","            self._create_features(example)\n","\n","        label = \" \".join(label)\n","        input_words = \" \".join(input_words)\n","        \n","        return(input_words, label)\n","    \n","    def _create_features(self, example):\n","    \n","        ingredients_ = \",\".join(example.ingredients)\n","        name_ = example.name\n","        \n","        label_ = \";\".join(example.steps) \n","                                        \n","        input_ = \"name: %s  ingredients: %s masked: %s </s>\" % (name_, ingredients_, masked_)\n","        target =  label_ + \" </s>\"\n","        \n","        # tokenize inputs\n","        tokenized_inputs = self.tokenizer.batch_encode_plus(\n","            [input_], max_length=self.max_len, padding = 'longest', return_tensors=\"pt\"\n","        )\n","        # tokenize targets\n","        tokenized_targets = self.tokenizer.batch_encode_plus(\n","            [target], max_length=60, padding = 'longest', return_tensors=\"pt\"\n","        )\n","\n","        self.inputs.append(tokenized_inputs)\n","        self.targets.append(tokenized_targets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0qt0rQhpEXet","executionInfo":{"status":"ok","timestamp":1620327781075,"user_tz":-330,"elapsed":55111,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}}},"source":["def get_dataset(tokenizer, type_path, args):\n","  return RecipeDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"b6VkWjWJEXet","executionInfo":{"status":"ok","timestamp":1620327781076,"user_tz":-330,"elapsed":55108,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}}},"source":["def tokenizer_test(type_path='train', mask_percent=0.4):\n","  '''\n","  ---------\n","  Test the dataset processing pipeline\n","  ---------\n","  '''\n","  tokenizer = T5Tokenizer.from_pretrained('t5-base')\n","  dataset = RecipeDataset(tokenizer, \n","                          data_dir='drive/MyDrive/recipe-generation/data/', \n","                          type_path=type_path, \n","                          mask_percent=mask_percent)\n","  len(dataset)\n","  \n","  return(dataset, tokenizer)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"D--mmpd4FrRo","executionInfo":{"status":"ok","timestamp":1620327781077,"user_tz":-330,"elapsed":55103,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}}},"source":["def display_data(dataset, tokenizer, n=250):\n","  '''\n","  ---------\n","  Display decoded dataset input output pair \n","  after passing through tokenizer\n","  ---------\n","  '''\n","  # Pick an element out from the dataset\n","  data = dataset[n]\n","  \n","  # Print output to make sure everything is all right\n","  print(data)\n","  print(tokenizer.decode(data['source_ids']))\n","  print(\"Target IDs\")\n","  print(tokenizer.decode(data['target_ids']))"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"6g72gxzgXHkl","executionInfo":{"status":"ok","timestamp":1620327781077,"user_tz":-330,"elapsed":55100,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}}},"source":["def tokenizer_version_test():\n","  '''\n","  -----------\n","  Simple test to recreate bug\n","  in old t5 tokenizer\n","  -----------\n","  '''\n","  tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","  text = \"The dog <extra_id_1> in the park\"\n","  tokenized_text = tokenizer.tokenize(text)\n","  print (tokenized_text)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"rE3e-pEqGrD0"},"source":["def generate_loop(model, tokenizer):\n","  '''\n","  ---------\n","  Generation loop\n","  --------\n","  '''\n","  dataset =  RecipeDataset(tokenizer, data_dir='drive/MyDrive/recipe-generation/data', type_path='val')\n","  loader = DataLoader(dataset, batch_size=32, num_workers=2)\n","\n","  model.model.eval()\n","  outputs = []\n","  targets = []\n","  \n","  for batch in tqdm(loader):\n","\n","    # Need to change max. length argument\n","    outs = model.model.generate(input_ids=batch['source_ids'], \n","                                attention_mask=batch['source_mask'], \n","                                max_length=60)\n","    dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n","    target = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[\"target_ids\"]]\n","\n","    print(dec)\n","    print(target)\n","  \n","    outputs.extend(dec)\n","    targets.extend(target)\n","  \n","  return(outputs, targets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"haZKLuNfEXev","executionInfo":{"status":"ok","timestamp":1620327781081,"user_tz":-330,"elapsed":55092,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}},"outputId":"275cd34c-5625-4710-d9d3-66057ba32c23"},"source":["args_dict = dict(\n","    data_dir=\"\", # path for data files\n","    output_dir=\"\", # path to save the checkpoints\n","    model_name_or_path='t5-base',\n","    tokenizer_name_or_path='t5-base',\n","    max_seq_length=512,\n","    learning_rate=3e-4,\n","    weight_decay=0.2,\n","    adam_epsilon=1e-8,\n","    warmup_steps=0,\n","    train_batch_size=8,\n","    eval_batch_size=8,\n","    num_train_epochs=2,\n","    gradient_accumulation_steps=1,\n","    n_gpu=1,\n","    early_stop_callback=True,\n","    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n","    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n","    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n","    seed=42,\n",")\n","\n","args_dict.update({'data_dir': 'drive/MyDrive/recipe-generation/data', \n","                  'output_dir': 'drive/MyDrive/recipe-generation/t5_fill_blanks', \n","                  'num_train_epochs': 3})\n","args = argparse.Namespace(**args_dict)\n","print(args_dict)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["{'data_dir': 'drive/MyDrive/recipe-generation/data', 'output_dir': 'drive/MyDrive/recipe-generation/t5_fill_blanks', 'model_name_or_path': 't5-base', 'tokenizer_name_or_path': 't5-base', 'max_seq_length': 512, 'learning_rate': 0.0003, 'weight_decay': 0.2, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'train_batch_size': 8, 'eval_batch_size': 8, 'num_train_epochs': 3, 'gradient_accumulation_steps': 1, 'n_gpu': 1, 'early_stop_callback': True, 'fp_16': False, 'opt_level': 'O1', 'max_grad_norm': 1.0, 'seed': 42}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gJZtJnd7EXev","executionInfo":{"status":"ok","timestamp":1620327781081,"user_tz":-330,"elapsed":55082,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}},"outputId":"e09e0686-de1e-41b4-f0c1-fe2d81e44294"},"source":["checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","    dirpath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5, \n",")\n","\n","train_params = dict(\n","    accumulate_grad_batches=args.gradient_accumulation_steps,\n","    gpus=args.n_gpu,\n","    max_epochs=args.num_train_epochs,\n","    # early_stop_callback=True,\n","    precision= 16 if args.fp_16 else 32,\n","    amp_level=args.opt_level,\n","    gradient_clip_val=args.max_grad_norm,\n","    checkpoint_callback=checkpoint_callback,\n","    callbacks=[LoggingCallback()],\n",")"],"execution_count":19,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: Checkpoint directory drive/MyDrive/recipe-generation/t5_fill_blanks exists and is not empty.\n","  warnings.warn(*args, **kwargs)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8WDMo96lEXev"},"source":["# Training loop \n","model = T5FineTuner(args)\n","trainer = pl.Trainer(**train_params)\n","trainer.fit(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JUJAehQcDUeo","executionInfo":{"status":"aborted","timestamp":1620328334037,"user_tz":-330,"elapsed":608000,"user":{"displayName":"Akshat Goel","photoUrl":"","userId":"10930603718103829639"}}},"source":["gen = False\n","if gen: \n","  from tqdm import tqdm\n","  model = T5FineTuner(args)\n","  tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","  generate_loop(model, tokenizer)"],"execution_count":null,"outputs":[]}]}