{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5_custom.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfCfH_lnEXeZ",
        "outputId": "98b88b17-6cf8-4363-ce94-f841a0bbab6c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIeRMOg3EXee",
        "outputId": "47ef911d-98ee-47b0-eedd-356edc079c62"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri May  7 20:00:09 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG5Fal3xEXef",
        "outputId": "15ee32a5-f55a-403e-fe8c-5271343d3507"
      },
      "source": [
        "# Install these packages with these specific versions else the notebook breaks\n",
        "!pip install transformers==4.5.1\n",
        "!pip install pytorch_lightning==1.2.10\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.5.1 in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (3.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (0.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.1) (3.4.1)\n",
            "Requirement already satisfied: pytorch_lightning==1.2.10 in /usr/local/lib/python3.7/dist-packages (1.2.10)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10) (1.8.1+cu101)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10) (4.41.1)\n",
            "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10) (5.3.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10) (1.19.5)\n",
            "Requirement already satisfied: torchmetrics==0.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10) (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10) (20.9)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10) (0.18.2)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10) (2021.4.0)\n",
            "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning==1.2.10) (2.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch_lightning==1.2.10) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch_lightning==1.2.10) (2.4.7)\n",
            "Requirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (2.23.0)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (3.7.4.post0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (1.28.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (56.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (0.4.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (0.36.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (1.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (3.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (1.6.3)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (20.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning==1.2.10) (5.1.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (3.10.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning==1.2.10) (3.1.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKKEhtH6EXeg",
        "outputId": "15a203fe-c126-40fa-eca2-64e8eb34f3f7"
      },
      "source": [
        "# Import packages\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "import sentencepiece\n",
        "\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll0uvgpIEXei"
      },
      "source": [
        "import csv\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from enum import Enum\n",
        "from typing import List, Optional\n",
        "from transformers import PreTrainedTokenizer"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IQo2COLEXej"
      },
      "source": [
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgLpsMkqEXej"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LoggingCallback(pl.Callback):\n",
        "  def on_validation_end(self, trainer, pl_module):\n",
        "    logger.info(\"***** Validation results *****\")\n",
        "    if pl_module.is_logger():\n",
        "      metrics = trainer.callback_metrics\n",
        "      # Log and save results to file\n",
        "      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"val_results.txt\")\n",
        "      with open(output_test_results_file, \"w\") as writer:\n",
        "        for key in sorted(metrics):\n",
        "          if key not in [\"log\", \"progress_bar\"]:\n",
        "            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "  def on_test_end(self, trainer, pl_module):\n",
        "    logger.info(\"***** Test results *****\")\n",
        "\n",
        "    if pl_module.is_logger():\n",
        "      metrics = trainer.callback_metrics\n",
        "\n",
        "      # Log and save results to file\n",
        "      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
        "      with open(output_test_results_file, \"w\") as writer:\n",
        "        for key in sorted(metrics):\n",
        "          if key not in [\"log\", \"progress_bar\"]:\n",
        "            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgrDdkhREXek"
      },
      "source": [
        "class T5FineTuner(pl.LightningModule):\n",
        "  def __init__(self, hparams):\n",
        "    super(T5FineTuner, self).__init__()\n",
        "    \n",
        "    if type(hparams) is dict: \n",
        "      hparams = argparse.Namespace(**hparams)\n",
        "    \n",
        "    self.hparams = hparams\n",
        "    \n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
        "    self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
        "  \n",
        "  def is_logger(self):\n",
        "    return self.trainer.global_rank <= 0\n",
        "  \n",
        "  def forward(\n",
        "      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n",
        "  ):\n",
        "    return self.model(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        decoder_input_ids=decoder_input_ids,\n",
        "        decoder_attention_mask=decoder_attention_mask,\n",
        "        labels=labels,\n",
        "    )\n",
        "\n",
        "  def _step(self, batch):\n",
        "    labels = batch[\"target_ids\"]\n",
        "    labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    outputs = self(\n",
        "        input_ids=batch[\"source_ids\"],\n",
        "        attention_mask=batch[\"source_mask\"],\n",
        "        labels=labels,\n",
        "        decoder_attention_mask=batch['target_mask']\n",
        "    )\n",
        "\n",
        "    loss = outputs[0]\n",
        "    \n",
        "    return loss\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    loss = self._step(batch)\n",
        "    self.log('training_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "    tensorboard_logs = {\"train_loss\": loss}\n",
        "    return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "  \n",
        "  def training_epoch_end(self, outputs):\n",
        "    avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "    self.log('avg_training_loss', avg_train_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "    tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
        "    return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    loss = self._step(batch)\n",
        "    self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "    tensorboard_logs = {\"val_loss\": loss}\n",
        "    return {\"val_loss\": loss, \"log\": tensorboard_logs}\n",
        "  \n",
        "  def validation_epoch_end(self, outputs):\n",
        "    avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "    print(avg_loss)\n",
        "    tensorboard_logs = {\"val_loss\": avg_loss}\n",
        "    self.log('avg_val_loss', avg_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
        "\n",
        "    model = self.model\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": self.hparams.weight_decay,\n",
        "        },\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "    self.opt = optimizer\n",
        "    return [optimizer]\n",
        "  \n",
        "  def optimizer_step(self,\n",
        "                     epoch=None, \n",
        "                     batch_idx=None, \n",
        "                     optimizer=None, \n",
        "                     optimizer_idx=None, \n",
        "                     optimizer_closure=None, \n",
        "                     on_tpu=None, \n",
        "                     using_native_amp=None, \n",
        "                     using_lbfgs=None\n",
        "                     ):\n",
        "\n",
        "    optimizer.step(closure=optimizer_closure)\n",
        "    optimizer.zero_grad()\n",
        "    self.lr_scheduler.step()\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
        "    dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n",
        "    t_total = (\n",
        "        (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
        "        // self.hparams.gradient_accumulation_steps\n",
        "        * float(self.hparams.num_train_epochs)\n",
        "    )\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "    self.lr_scheduler = scheduler\n",
        "    return dataloader\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n",
        "    return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUR4zS45EXeo"
      },
      "source": [
        "# The below code is adapted from:\n",
        "# https://github.com/huggingface/transformers/blob/master/examples/multiple-choice/utils_multiple_choice.py\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class InputExample:\n",
        "    \"\"\"\n",
        "    A single training/test example for multiple choice\n",
        "    Args:\n",
        "        example_id: Unique id for the example.\n",
        "        question: string. The untokenized text of the second sequence (question).\n",
        "        contexts: list of str. The untokenized text of the first sequence (context of corresponding question).\n",
        "        endings: list of str. multiple choice's options. Its length must be equal to contexts' length.\n",
        "        label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "\n",
        "    name: str\n",
        "    ingredients: str\n",
        "    steps: List[str]\n",
        "    label: Optional[str]\n",
        "\n",
        "class Split(Enum):\n",
        "    train = \"train\"\n",
        "    dev = \"dev\"\n",
        "    test = \"test\"\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Base class for data converters for multiple choice data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "class RecipeProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the SWAG data set.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        logger.info(\"LOOKING AT {} train\".format(data_dir))\n",
        "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n",
        "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"val.csv\")), \"dev\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n",
        "        raise ValueError(\n",
        "            \"For swag testing, the input file does not contain a label column. It can not be tested in current code\"\n",
        "            \"setting!\"\n",
        "        )\n",
        "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"test.csv\")), \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"0\", \"1\", \"2\", \"3\"]\n",
        "\n",
        "    def _read_csv(self, input_file):\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            return list(csv.reader(f))\n",
        "\n",
        "    def _create_examples(self, lines: List[List[str]], type: str):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "\n",
        "        examples = [\n",
        "            InputExample(\n",
        "                name=line[1],\n",
        "                # common beginning of each\n",
        "                # choice is stored in \"sent2\".\n",
        "                ingredients=eval(line[2]),\n",
        "                steps=eval(line[3]),\n",
        "                label=eval(line[3]),\n",
        "            )\n",
        "            for line in lines[1:]  # we skip the line with the column names\n",
        "        ]\n",
        "\n",
        "        return examples"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbcg-MsjEXer"
      },
      "source": [
        "class RecipeDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, tokenizer, data_dir, type_path,  max_len=512, mask_percent=0.4):\n",
        "        self.data_dir = data_dir\n",
        "        self.type_path = type_path\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "        self.mask_percent = mask_percent\n",
        "        \n",
        "        self.proc = RecipeProcessor()\n",
        "        self._build()\n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "\n",
        "        src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \n",
        "                \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "  \n",
        "    def _build(self):\n",
        "        if self.type_path == 'train':\n",
        "            examples = self.proc.get_train_examples(self.data_dir)\n",
        "        else:\n",
        "            examples = self.proc.get_dev_examples(self.data_dir)\n",
        "    \n",
        "        for i, example in enumerate(examples):\n",
        "            if i % 10000 == 0:\n",
        "                print(i)\n",
        "            self._create_features(example)\n",
        "  \n",
        "    def _create_masks(self, steps):\n",
        "        \n",
        "        words = [word for step in steps for word in step.split(\" \")]\n",
        "        \n",
        "        total_words = len(words)\n",
        "        if total_words == 0:\n",
        "          mask_words = 0\n",
        "        elif total_words < 100:\n",
        "          mask_words = np.round(self.mask_percent*total_words).astype(int)\n",
        "        else:\n",
        "          mask_words = 50\n",
        "        mask_indices = np.random.choice(np.arange(total_words), mask_words)\n",
        "        \n",
        "        input_words = []\n",
        "        label = []\n",
        "        j = 0\n",
        "        \n",
        "        for i, word in enumerate(words):\n",
        "            if i in mask_indices:\n",
        "                # Add a sentinel token in place of the word to be masked\n",
        "                input_words.append('<extra_id_{}>'.format(j))\n",
        "                # Add this word's token to the label list\n",
        "                label.append('<extra_id_{}>'.format(j))\n",
        "                # Add the corresponding label to the list\n",
        "                label.append(word)\n",
        "                # Step the counter by 1\n",
        "                j+=1\n",
        "            # If this index is not among the sampled indices \n",
        "            # just append the word\n",
        "            else:\n",
        "                input_words.append(word)\n",
        "         \n",
        "        # Add in an extra ID token like the format says\n",
        "        label.append('<extra_id_{}>'.format(j))\n",
        "\n",
        "        label = \" \".join(label)\n",
        "        input_words = \" \".join(input_words)\n",
        "        \n",
        "        return(input_words, label)\n",
        "    \n",
        "    def _create_features(self, example):\n",
        "    \n",
        "        ingredients_ = \",\".join(example.ingredients)\n",
        "        name_ = example.name\n",
        "        \n",
        "        masked_, label_ = self._create_masks(example.steps) \n",
        "                                        \n",
        "        input_ = \"name: %s  ingredients: %s masked: %s </s>\" % (name_, ingredients_, masked_)\n",
        "        target =  label_ + \" </s>\"\n",
        "        \n",
        "        # tokenize inputs\n",
        "        tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "            [input_], padding='longest', return_tensors=\"pt\"\n",
        "        )\n",
        "        # tokenize targets\n",
        "        tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "            [target], padding='longest', return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        self.inputs.append(tokenized_inputs)\n",
        "        self.targets.append(tokenized_targets)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dKj3KplGmJZ"
      },
      "source": [
        "class RecipeGenDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, tokenizer, data_dir, type_path,  max_len=512, mask_percent=0.4):\n",
        "        self.data_dir = data_dir\n",
        "        self.type_path = type_path\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "        self.mask_percent = mask_percent\n",
        "        \n",
        "        self.proc = RecipeProcessor()\n",
        "        self._build()\n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "\n",
        "        src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \n",
        "                \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "  \n",
        "    def _build(self):\n",
        "        if self.type_path == 'train':\n",
        "            examples = self.proc.get_train_examples(self.data_dir)\n",
        "        else:\n",
        "            examples = self.proc.get_dev_examples(self.data_dir)\n",
        "    \n",
        "        for i, example in enumerate(examples):\n",
        "            if i % 10000 == 0:\n",
        "                print(i)\n",
        "            self._create_features(example)\n",
        "\n",
        "        label = \" \".join(label)\n",
        "        input_words = \" \".join(input_words)\n",
        "        \n",
        "        return(input_words, label)\n",
        "    \n",
        "    def _create_features(self, example):\n",
        "    \n",
        "        ingredients_ = \",\".join(example.ingredients)\n",
        "        name_ = example.name\n",
        "        \n",
        "        label_ = \";\".join(example.steps) \n",
        "                                        \n",
        "        input_ = \"name: %s  ingredients: %s masked: %s </s>\" % (name_, ingredients_, masked_)\n",
        "        target =  label_ + \" </s>\"\n",
        "        \n",
        "        # tokenize inputs\n",
        "        tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "            [input_], padding = 'longest', return_tensors=\"pt\"\n",
        "        )\n",
        "        # tokenize targets\n",
        "        tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "            [target], padding = 'longest', return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        self.inputs.append(tokenized_inputs)\n",
        "        self.targets.append(tokenized_targets)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qt0rQhpEXet"
      },
      "source": [
        "def get_dataset(tokenizer, type_path, args):\n",
        "  return RecipeDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6VkWjWJEXet"
      },
      "source": [
        "def tokenizer_test(type_path='train', mask_percent=0.4):\n",
        "  '''\n",
        "  ---------\n",
        "  Test the dataset processing pipeline\n",
        "  ---------\n",
        "  '''\n",
        "  tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "  dataset = RecipeDataset(tokenizer, \n",
        "                          data_dir='drive/MyDrive/recipe-generation/data/', \n",
        "                          type_path=type_path, \n",
        "                          mask_percent=mask_percent)\n",
        "  len(dataset)\n",
        "  \n",
        "  return(dataset, tokenizer)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D--mmpd4FrRo"
      },
      "source": [
        "def display_data(dataset, tokenizer, n=250):\n",
        "  '''\n",
        "  ---------\n",
        "  Display decoded dataset input output pair \n",
        "  after passing through tokenizer\n",
        "  ---------\n",
        "  '''\n",
        "  # Pick an element out from the dataset\n",
        "  data = dataset[n]\n",
        "  \n",
        "  # Print output to make sure everything is all right\n",
        "  print(data)\n",
        "  print(tokenizer.decode(data['source_ids']))\n",
        "  print(\"Target IDs\")\n",
        "  print(tokenizer.decode(data['target_ids']))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g72gxzgXHkl"
      },
      "source": [
        "def tokenizer_version_test():\n",
        "  '''\n",
        "  -----------\n",
        "  Simple test to recreate bug\n",
        "  in old t5 tokenizer\n",
        "  -----------\n",
        "  '''\n",
        "  tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "  text = \"The dog <extra_id_1> in the park\"\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  print (tokenized_text)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE3e-pEqGrD0"
      },
      "source": [
        "def generate_loop(checkpoint_path='drive/MyDrive/recipe-generation/t5_fill_blanks/latest.ckpt'):\n",
        "  '''\n",
        "  ---------\n",
        "  Generation loop\n",
        "  --------\n",
        "  '''\n",
        "  model = T5FineTuner.load_from_checkpoint(checkpoint_path)\n",
        "  tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "  dataset =  RecipeDataset(tokenizer, data_dir='drive/MyDrive/recipe-generation/data', type_path='val')\n",
        "  loader = DataLoader(dataset, batch_size=32, num_workers=2)\n",
        "\n",
        "  model.model.eval()\n",
        "  outputs = []\n",
        "  targets = []\n",
        "\n",
        "  for batch in tqdm(loader):\n",
        "\n",
        "    # Need to change max. length argument\n",
        "    outs = model.model.generate(input_ids=batch['source_ids'], \n",
        "                                attention_mask=batch['source_mask'], \n",
        "                                max_length=60)\n",
        "    dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
        "    target = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[\"target_ids\"]]\n",
        "\n",
        "    print(dec)\n",
        "    print(target)\n",
        "  \n",
        "    outputs.extend(dec)\n",
        "    targets.extend(target)\n",
        "  \n",
        "  return(outputs, targets) "
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haZKLuNfEXev",
        "outputId": "b04dd21b-351e-4c53-a02a-8e9d607cd2f7"
      },
      "source": [
        "args_dict = dict(\n",
        "    data_dir='drive/MyDrive/recipe-generation/data', # path for data files\n",
        "    output_dir='drive/MyDrive/recipe-generation/t5_fill_blanks', # path to save the checkpoints\n",
        "    model_name_or_path='t5-small',\n",
        "    tokenizer_name_or_path='t5-base',\n",
        "    max_seq_length=512,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.5,\n",
        "    adam_epsilon=1e-8,\n",
        "    warmup_steps=0,\n",
        "    train_batch_size=8,\n",
        "    eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    gradient_accumulation_steps=1,\n",
        "    n_gpu=1,\n",
        "    early_stop_callback=True,\n",
        "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
        "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
        "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
        "    seed=42,\n",
        "    cp_filename='latest'\n",
        ")\n",
        "\n",
        "args_dict.update({'data_dir': 'drive/MyDrive/recipe-generation/data', \n",
        "                  'output_dir': 'drive/MyDrive/recipe-generation/t5_fill_blanks', \n",
        "                  'num_train_epochs': 4})\n",
        "args = argparse.Namespace(**args_dict)\n",
        "print(args_dict)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'data_dir': 'drive/MyDrive/recipe-generation/data', 'output_dir': 'drive/MyDrive/recipe-generation/t5_fill_blanks', 'model_name_or_path': 't5-small', 'tokenizer_name_or_path': 't5-base', 'max_seq_length': 512, 'learning_rate': 0.0003, 'weight_decay': 0.5, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'train_batch_size': 8, 'eval_batch_size': 8, 'num_train_epochs': 4, 'gradient_accumulation_steps': 1, 'n_gpu': 1, 'early_stop_callback': True, 'fp_16': False, 'opt_level': 'O1', 'max_grad_norm': 1.0, 'seed': 42, 'cp_filename': 'latest'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJZtJnd7EXev",
        "outputId": "45294256-7e52-45d1-8622-63c9d7987904"
      },
      "source": [
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    dirpath=args.output_dir, monitor=\"avg_val_loss\", mode=\"min\", save_top_k=1, \n",
        "    filename=args.cp_filename\n",
        ")\n",
        "\n",
        "train_params = dict(\n",
        "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    gpus=args.n_gpu,\n",
        "    max_epochs=args.num_train_epochs,\n",
        "    precision= 16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    gradient_clip_val=args.max_grad_norm,\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    callbacks=[LoggingCallback()],\n",
        "    val_check_interval=0.5,\n",
        ")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: Checkpoint directory drive/MyDrive/recipe-generation/t5_fill_blanks exists and is not empty.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WDMo96lEXev"
      },
      "source": [
        "# Training loop \n",
        "model = T5FineTuner(args)\n",
        "trainer = pl.Trainer(**train_params)\n",
        "trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUJAehQcDUeo"
      },
      "source": [
        "gen = False\n",
        "checkpoint_path = os.path.join(args.output_dict, 'latest.ckpt')\n",
        "if gen: \n",
        "  from tqdm import tqdm\n",
        "  model = T5FineTuner(args)\n",
        "  tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "  generate_loop(checkpoint_path)"
      ],
      "execution_count": 79,
      "outputs": []
    }
  ]
}