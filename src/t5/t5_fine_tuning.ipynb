{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJX4vkjj6wYz",
    "outputId": "df9ad677-b5e0-4a04-e6e6-59114be6778c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1V5cInhu42Wk",
    "outputId": "8dbf1005-42f9-4500-901a-cb00fcb16e89"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epWcPHhJ7v7j"
   },
   "source": [
    "Instal apex if you want to do 16 bit training. You'll probably need to restart the notebook after installing apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1Xy7ZG-7gHt"
   },
   "outputs": [],
   "source": [
    "# !export CUDA_HOME=/usr/local/cuda-10.1\n",
    "# !git clone https://github.com/NVIDIA/apex\n",
    "# !pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SDVQ04fGRb1v",
    "outputId": "c8698d7e-9784-42bc-eb15-f6f86fd0f5ce"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==2.9.0\n",
    "!pip install pytorch_lightning==0.7.5\n",
    "!pip install sentencepiece==0.1.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVxGfmEMCKs_"
   },
   "source": [
    "## T5 fine-tuning\n",
    "\n",
    "This notebook is to showcase how to fine-tune [T5 model](https://arxiv.org/abs/1910.10683) with Huggigface's [Transformers](https://github.com/huggingface/transformers/) to solve different NLP tasks using text-2-text approach proposed in the T5 paper. For demo I chose 3 non text-2-text problems just to reiterate the fact from the paper that how widely applicable this text-2-text framework is and how it can be used for different tasks without changing the model at all.\n",
    "\n",
    "This is a rough draft so if you find any issues with this notebook or have any  questions reach out to me via [Twitter](https://twitter.com/psuraj28).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HS8mNXq6bdxq",
    "outputId": "6595148e-89ce-48c4-ad44-2acc818014f1"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import sentencepiece\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IswYuhWaz7QJ"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKNr7fgzcKpZ"
   },
   "source": [
    "## Model\n",
    "\n",
    "We'll be using the awesome [pytorch-lightning](https://github.com/PytorchLightning/pytorch-lightning) library for training. Most of the below code is adapted from here https://github.com/huggingface/transformers/blob/master/examples/lightning_base.py\n",
    "\n",
    "The trainer is generic and can be used for any text-2-text task. You'll just need to change the dataset. Rest of the code will stay unchanged for all the tasks.\n",
    "\n",
    "This is the most intresting and powrfull thing about the text-2-text format. You can fine-tune the model on variety of NLP tasks by just formulating the problem in text-2-text setting. No need to change hyperparameters, learning rate, optimizer or loss function. Just plug in your dataset and you are ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7uVNBtXST5X"
   },
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "  def __init__(self, hparams):\n",
    "    super(T5FineTuner, self).__init__()\n",
    "    self.hparams = hparams\n",
    "    \n",
    "    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "    self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "  \n",
    "  def is_logger(self):\n",
    "    return self.trainer.proc_rank <= 0\n",
    "  \n",
    "  def forward(\n",
    "      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
    "  ):\n",
    "    return self.model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        decoder_attention_mask=decoder_attention_mask,\n",
    "        lm_labels=lm_labels,\n",
    "    )\n",
    "\n",
    "  def _step(self, batch):\n",
    "    lm_labels = batch[\"target_ids\"]\n",
    "    lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    outputs = self(\n",
    "        input_ids=batch[\"source_ids\"],\n",
    "        attention_mask=batch[\"source_mask\"],\n",
    "        lm_labels=lm_labels,\n",
    "        decoder_attention_mask=batch['target_mask']\n",
    "    )\n",
    "\n",
    "    loss = outputs[0]\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "\n",
    "    tensorboard_logs = {\"train_loss\": loss}\n",
    "    return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "  \n",
    "  def training_epoch_end(self, outputs):\n",
    "    avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "    tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "    return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss = self._step(batch)\n",
    "    return {\"val_loss\": loss}\n",
    "  \n",
    "  def validation_epoch_end(self, outputs):\n",
    "    avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "    tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "    return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "    model = self.model\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": self.hparams.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "    self.opt = optimizer\n",
    "    return [optimizer]\n",
    "  \n",
    "  def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
    "    if self.trainer.use_tpu:\n",
    "      xm.optimizer_step(optimizer)\n",
    "    else:\n",
    "      optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    self.lr_scheduler.step()\n",
    "  \n",
    "  def get_tqdm_dict(self):\n",
    "    tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "    return tqdm_dict\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n",
    "    dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n",
    "    t_total = (\n",
    "        (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
    "        // self.hparams.gradient_accumulation_steps\n",
    "        * float(self.hparams.num_train_epochs)\n",
    "    )\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "    self.lr_scheduler = scheduler\n",
    "    return dataloader\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n",
    "    return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oh1R5C-GwMqx"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "  def on_validation_end(self, trainer, pl_module):\n",
    "    logger.info(\"***** Validation results *****\")\n",
    "    if pl_module.is_logger():\n",
    "      metrics = trainer.callback_metrics\n",
    "      # Log results\n",
    "      for key in sorted(metrics):\n",
    "        if key not in [\"log\", \"progress_bar\"]:\n",
    "          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "  def on_test_end(self, trainer, pl_module):\n",
    "    logger.info(\"***** Test results *****\")\n",
    "\n",
    "    if pl_module.is_logger():\n",
    "      metrics = trainer.callback_metrics\n",
    "\n",
    "      # Log and save results to file\n",
    "      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
    "      with open(output_test_results_file, \"w\") as writer:\n",
    "        for key in sorted(metrics):\n",
    "          if key not in [\"log\", \"progress_bar\"]:\n",
    "            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4hjvsBJ5Zk5"
   },
   "source": [
    "Let's define the hyperparameters and other arguments. You can overide this `dict` for specific task as needed. While in most of cases you'll only need to change the `data_dir`and `output_dir`.\n",
    "\n",
    "Here the batch size is 8 and gradient_accumulation_steps are 16 so the effective batch size is 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urduopvizqTq"
   },
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    data_dir=\"\", # path for data files\n",
    "    output_dir=\"\", # path to save the checkpoints\n",
    "    model_name_or_path='t5-base',\n",
    "    tokenizer_name_or_path='t5-base',\n",
    "    max_seq_length=512,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfhlYUUV2NIh"
   },
   "source": [
    "## IMDB review classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3C13iabZvwK"
   },
   "source": [
    "### Download IMDB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7R0QdcgXuIWW",
    "outputId": "80f29c69-946b-41bc-f6d3-20cb49c05c13"
   },
   "outputs": [],
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xvf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ni1cAK7EvXSB"
   },
   "outputs": [],
   "source": [
    "train_pos_files = glob.glob('aclImdb/train/pos/*.txt')\n",
    "train_neg_files = glob.glob('aclImdb/train/neg/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEsRn5pa0v8d",
    "outputId": "666c6fd4-3cbc-46df-aa32-3581a47e7149"
   },
   "outputs": [],
   "source": [
    "len(train_pos_files), len(train_neg_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zgS8KhlaPiA"
   },
   "source": [
    "We will use 2000 samples from the train set for validation. Let's choose 1000 postive reviews and 1000 negative reviews for validation and save them in the val directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLvBHcXwzXrk",
    "outputId": "e01723df-8be6-4b33-d76b-53f5a6641d1f"
   },
   "outputs": [],
   "source": [
    "!mkdir aclImdb/val aclImdb/val/pos aclImdb/val/neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXZmLZ1pzjiY"
   },
   "outputs": [],
   "source": [
    "random.shuffle(train_pos_files)\n",
    "random.shuffle(train_neg_files)\n",
    "\n",
    "val_pos_files = train_pos_files[:1000]\n",
    "val_neg_files = train_neg_files[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yTS2Jx40UNu"
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "hJnJpkdb0ZKY",
    "outputId": "5bab7487-de3f-49d0-91ed-e4fa44aaa5bb"
   },
   "outputs": [],
   "source": [
    "for f in val_pos_files:\n",
    "  shutil.move(f,  'aclImdb/val/pos')\n",
    "for f in val_neg_files:\n",
    "  shutil.move(f,  'aclImdb/val/neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdEgCwL7cIyi"
   },
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205,
     "referenced_widgets": [
      "c66f16e3f218442386d3c6137aea6824",
      "9e868445191a4546b914e370e3a3653a",
      "aa844d6a281f4c18be3be48ec30a2091",
      "06267db87bf142058775dddf3f589299",
      "863720f4f8834cda8bd0de0197663e63",
      "947d04f0c06c4464acfec292e9e78ba0",
      "5b6828f6efe847a7a90ec48b03f76a71",
      "46a2e458d20f403587bee733a30c8f9c"
     ]
    },
    "id": "McQC1FotigqA",
    "outputId": "d29e8793-4d13-4c1f-843a-5cd82488f32e"
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wthd9SM74RG8",
    "outputId": "7b393195-da92-48f5-d0e2-5cab3ed8fc7b"
   },
   "outputs": [],
   "source": [
    "ids_neg = tokenizer.encode('negative </s>')\n",
    "ids_pos = tokenizer.encode('positive </s>')\n",
    "len(ids_neg), len(ids_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5sJkyI3a723"
   },
   "source": [
    "All the examples are converted in the text-2-text format as shown in the paper. However I didn't use any task prefix here. The examples are encoded as follows,\n",
    "if the review is positive then the target is 'positive' else 'negative'\n",
    "\n",
    "**input**:  I went to see this\n",
    "movie with my husband, and we both\n",
    "thought the acting was terrible!\"\n",
    "\n",
    "**target**: negative\n",
    "\n",
    "**input**:  Despite what others say,\n",
    "I thought this movie was funny.\n",
    "\n",
    "**target**: positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEYmYHKGcxEq"
   },
   "source": [
    "The dataset below takes care of reading the review files and processing the examples in text-2-text format.\n",
    "\n",
    "It cleans the review text by removing the html tags. It also appends the eos token `</s>` at the end of input and target as required by the T5 model \n",
    "\n",
    "For T5 max input length is 512 and we can choose the max length for target sequence depending upon our dataset. The `T5Tokenizer` encodes both 'postive' and 'negative' as a single ids so I chose the max target length 2, extra 1 for the `</s>` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIY0GenSb72m"
   },
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "  def __init__(self, tokenizer, data_dir, type_path,  max_len=512):\n",
    "    self.pos_file_path = os.path.join(data_dir, type_path, 'pos')\n",
    "    self.neg_file_path = os.path.join(data_dir, type_path, 'neg')\n",
    "    \n",
    "    self.pos_files = glob.glob(\"%s/*.txt\" % self.pos_file_path)\n",
    "    self.neg_files = glob.glob(\"%s/*.txt\" % self.neg_file_path)\n",
    "    \n",
    "    self.max_len = max_len\n",
    "    self.tokenizer = tokenizer\n",
    "    self.inputs = []\n",
    "    self.targets = []\n",
    "\n",
    "    self._build()\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "    target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "    src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "    target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "    return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "  \n",
    "  def _build(self):\n",
    "    self._buil_examples_from_files(self.pos_files, 'positive')\n",
    "    self._buil_examples_from_files(self.neg_files, 'negative')\n",
    "  \n",
    "  def _buil_examples_from_files(self, files, sentiment):\n",
    "    REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "    for path in files:\n",
    "      with open(path, 'r') as f:\n",
    "        text = f.read()\n",
    "      \n",
    "      line = text.strip()\n",
    "      line = REPLACE_NO_SPACE.sub(\"\", line) \n",
    "      line = REPLACE_WITH_SPACE.sub(\"\", line)\n",
    "      line = line + ' </s>'\n",
    "\n",
    "      target = sentiment + \" </s>\"\n",
    "\n",
    "       # tokenize inputs\n",
    "      tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "          [line], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "      )\n",
    "       # tokenize targets\n",
    "      tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "          [target], max_length=2, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "      )\n",
    "\n",
    "      self.inputs.append(tokenized_inputs)\n",
    "      self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsnsKY6jemsr",
    "outputId": "f5ae675a-adb7-449b-b9da-8032d57f1309"
   },
   "outputs": [],
   "source": [
    "dataset = ImdbDataset(tokenizer, 'aclImdb', 'val',  max_len=512)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7g1gz05ccAzg",
    "outputId": "3f39b5fb-3371-4cbd-a456-0901f7dae9ab"
   },
   "outputs": [],
   "source": [
    "data = dataset[28]\n",
    "print(tokenizer.decode(data['source_ids']))\n",
    "print(tokenizer.decode(data['target_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4cfw8bMcNdA"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTvkv4rzhPjy"
   },
   "outputs": [],
   "source": [
    "!mkdir -p t5_imdb_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5ngAP4OXFqZ"
   },
   "outputs": [],
   "source": [
    "args_dict.update({'data_dir': 'aclImdb', 'output_dir': 't5_imdb_sentiment', 'num_train_epochs':2})\n",
    "args = argparse.Namespace(**args_dict)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    early_stop_callback=False,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJt_VqzEAMUg"
   },
   "source": [
    "Define the `get_dataset` function to return the dataset. The model calls this function to get the train and val datasets. We are defining a dataset function so that we won't need to modify the model code at all. Redefine the function to return different dataset according to the problem. While this is not the best solution for now this works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2h2aGPgp0vOf"
   },
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, type_path, args):\n",
    "  return ImdbDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IOQpawZA9XC"
   },
   "source": [
    "**Initialize model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "dbbbcab8656d4a189f8c647bf4298c9b",
      "9157b33faf284f3296e20450cc5e6d17",
      "764fc71349e04ca2bc56885684bc406b",
      "d99dbf5460e24a21a070df9b4f80ce27",
      "d76ae256aaf84e37a03d8bd423d49dd5",
      "b32951e9fbc64680b03d61235ee13bda",
      "9cdae1eac7cc4aa1a31137941fabda03",
      "24957342340b48ed9b6c8dc3dfe22a7e",
      "89c7a8ac42c24078a65e7afcfea5c2fe",
      "26da2def11c04013b99ba9fc06f0f97e",
      "be778f6359224ff9b9d1d99b0eb68ae0",
      "f276ed431c60493e8e166bc9c711c852",
      "ed90ad03272845f2bd803c59272d8b4c",
      "56f9052eb67147e7b2152503e2d67807",
      "ed97db83a8e649c0a495dc319d3d6fad",
      "b97fa18acaf5454a86540f14b52976be"
     ]
    },
    "id": "kJsz3a4SilAF",
    "outputId": "7c3e3511-235d-4d75-ae78-2ca7e129fb4e"
   },
   "outputs": [],
   "source": [
    "model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSJytKv1BFyc"
   },
   "source": [
    "**Initialize trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxO8OTA3irbw",
    "outputId": "f3c5c237-57da-4473-8d19-d782d1b575d0"
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wo7cSSvFGEhe"
   },
   "source": [
    "**start fine-tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f40fb3182787435d81fa9f910ba63a83",
      "124075f5a54c46aa8d488478c5155c3a",
      "3d263a629b3f408f8a72e9dcec5159cd",
      "cada440810c64395a0af0744ddbfcfdf",
      "d97b3deb4aaa44739cc228ae7bcdbc1b",
      "7ee2294de22c4cc380d0000e6e99dae5",
      "20d649a0fedd445f931f31e637c6f460",
      "dc5547e7bfe14eda9dc9fd58d907f7a7"
     ]
    },
    "id": "hVGd6imfizLP",
    "outputId": "9d2b74f1-2964-4218-8ba3-f69692d5593a"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-obOz6v70iB"
   },
   "outputs": [],
   "source": [
    "!mkdir t5_base_imdb_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQBJcrrWi2vC"
   },
   "outputs": [],
   "source": [
    "## save the model this way so next time you can load it using T5ForConditionalGeneration.from_pretrained\n",
    "model.model.save_pretrained('t5_base_imdb_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhjELPOk7-cz"
   },
   "outputs": [],
   "source": [
    "# !cp -r t5_base_imdb_sentiment drive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brPOSAkjNP5t"
   },
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7SuVh05lDrJ"
   },
   "source": [
    "For inference we will use the `generate` method with greedy decoding with max length 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25jbT49CVoXN"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyriGR20lSRa"
   },
   "source": [
    "Let's visualize few predictions on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwJ998sMz2Ci"
   },
   "outputs": [],
   "source": [
    "dataset = ImdbDataset(tokenizer, 'aclImdb', 'test',  max_len=512)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LQtN5b90TyW"
   },
   "outputs": [],
   "source": [
    "it = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRD03teH0YMe"
   },
   "outputs": [],
   "source": [
    "batch = next(it)\n",
    "batch[\"source_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eewDktozk7GN"
   },
   "outputs": [],
   "source": [
    "outs = model.model.generate(input_ids=batch['source_ids'].cuda(), \n",
    "                              attention_mask=batch['source_mask'].cuda(), \n",
    "                              max_length=2)\n",
    "\n",
    "dec = [tokenizer.decode(ids) for ids in outs]\n",
    "\n",
    "texts = [tokenizer.decode(ids) for ids in batch['source_ids']]\n",
    "targets = [tokenizer.decode(ids) for ids in batch['target_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vBe0UNw7cHY"
   },
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    lines = textwrap.wrap(\"Review:\\n%s\\n\" % texts[i], width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual sentiment: %s\" % targets[i])\n",
    "    print(\"Predicted sentiment: %s\" % dec[i])\n",
    "    print(\"=====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lATfuiHYHq_1"
   },
   "source": [
    "Now predict on all the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvWQGLXhzHtn"
   },
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=32, num_workers=4)\n",
    "model.model.eval()\n",
    "outputs = []\n",
    "targets = []\n",
    "for batch in tqdm(loader):\n",
    "  outs = model.model.generate(input_ids=batch['source_ids'].cuda(), \n",
    "                              attention_mask=batch['source_mask'].cuda(), \n",
    "                              max_length=2)\n",
    "\n",
    "  dec = [tokenizer.decode(ids) for ids in outs]\n",
    "  target = [tokenizer.decode(ids) for ids in batch[\"target_ids\"]]\n",
    "  \n",
    "  outputs.extend(dec)\n",
    "  targets.extend(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBxEcXeWGafd"
   },
   "source": [
    "Let's check if the model generates any invalid text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_qylwYGXgwY"
   },
   "outputs": [],
   "source": [
    "for i, out in enumerate(outputs):\n",
    "  if out not in ['positive', 'negative']:\n",
    "    print(i, 'detected invalid prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpU_VkFGIgnw"
   },
   "source": [
    "This great is great! Our model hasn't generated any invalid prediction. Let's calculate accuarcy and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdJcQODoOChP"
   },
   "outputs": [],
   "source": [
    "metrics.accuracy_score(targets, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YepnSgI5OKti"
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(targets, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcZqrJELrRVw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dhqigmiw2hVh"
   },
   "source": [
    "## Emotion classification\n",
    "\n",
    "While most of the sentiment-analysis datasets are binary with 'postive' and 'negative' sentiments, [Elvis Saravia](https://twitter.com/omarsar0)  has put together a great [dataset](https://github.com/dair-ai/emotion_dataset) for emotion recognition. The task is given some text classifiy the text into one of the following six emotions \n",
    "\n",
    "'sadness', 'joy', 'anger', 'fear', 'surprise', 'love'.\n",
    "\n",
    "Here's the [original notebook](https://colab.research.google.com/drive/1nwCE6b9PXIKhv2hvbqf1oZKIGkXMTi1X#scrollTo=pSzoz9InH0Ta) which trains ROBERTa model to classify the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0B4IhzEgO21B"
   },
   "source": [
    "### Download and view data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eQhtsD65svj"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt\n",
    "!wget https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt\n",
    "!wget https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVrcVbvx74G5"
   },
   "outputs": [],
   "source": [
    "!mkdir emotion_data\n",
    "!mv *.txt emotion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOpnh3Y06BGU"
   },
   "outputs": [],
   "source": [
    "train_path = \"emotion_data/train.txt\"\n",
    "test_path = \"emotion_data/test.txt\"\n",
    "val_path = \"emotion_data/val.txt\"\n",
    "\n",
    "## emotion labels\n",
    "label2int = {\n",
    "  \"sadness\": 0,\n",
    "  \"joy\": 1,\n",
    "  \"love\": 2,\n",
    "  \"anger\": 3,\n",
    "  \"fear\": 4,\n",
    "  \"surprise\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4sDek6T8PXE"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(train_path, sep=\";\", header=None, names=['text', 'emotion'],\n",
    "                               engine=\"python\")\n",
    "data.emotion.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaKp3E1T8kkm"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-Gt1WyPBL-6"
   },
   "outputs": [],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KybpXVl1Die5"
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cANrUEXhO8QY"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GsMQdqMPCN7"
   },
   "source": [
    "Here also we will process the examples in the same way we did above. If the label is 'love' we will ask the model to predict the text 'love'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKh6m92eKZc4"
   },
   "source": [
    "Lets check how t5 encodes the following labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDnMp5-fDIAc"
   },
   "outputs": [],
   "source": [
    "emotions = [ \"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "for em in emotions:\n",
    "  print(len(tokenizer.encode(em)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8VIZIWFOwMj"
   },
   "source": [
    "Here also all the labels are encoded as single ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8i8QD-3MDrWq"
   },
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "  def __init__(self, tokenizer, data_dir, type_path,  max_len=512):\n",
    "    self.path = os.path.join(data_dir, type_path + '.txt')\n",
    "\n",
    "    self.data_column = \"text\"\n",
    "    self.class_column = \"emotion\"\n",
    "    self.data = pd.read_csv(self.path, sep=\";\", header=None, names=[self.data_column, self.class_column],\n",
    "                            engine=\"python\")\n",
    "    \n",
    "    self.max_len = max_len\n",
    "    self.tokenizer = tokenizer\n",
    "    self.inputs = []\n",
    "    self.targets = []\n",
    "\n",
    "    self._build()\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "    target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "    src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "    target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "    return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "  \n",
    "  def _build(self):\n",
    "    for idx in range(len(self.data)):\n",
    "      input_, target = self.data.loc[idx, self.data_column], self.data.loc[idx, self.class_column]      \n",
    "      \n",
    "      input_ = input_ + ' </s>'\n",
    "      target = target + \" </s>\"\n",
    "\n",
    "       # tokenize inputs\n",
    "      tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "          [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "      )\n",
    "       # tokenize targets\n",
    "      tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "          [target], max_length=2, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "      )\n",
    "\n",
    "      self.inputs.append(tokenized_inputs)\n",
    "      self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRz5jyl3FBkv"
   },
   "outputs": [],
   "source": [
    "dataset = EmotionDataset(tokenizer, 'emotion_data', 'val', 512)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxT6QzUAFQN0"
   },
   "outputs": [],
   "source": [
    "data = dataset[42]\n",
    "print(tokenizer.decode(data['source_ids']))\n",
    "print(tokenizer.decode(data['target_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBVHtdIuFpID"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEWi6c-pGZV9"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGrpDJnLPQ0Q"
   },
   "source": [
    "As I said above there's no need to change the model or add task specific head or any other hyperparameters, we'll just change the dataset and that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDep-uIcGYX2"
   },
   "outputs": [],
   "source": [
    "!mkdir -p t5_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgNOy7a4LJ9h"
   },
   "outputs": [],
   "source": [
    "args_dict.update({'data_dir': 'emotion_data', 'output_dir': 't5_emotion', 'num_train_epochs':2})\n",
    "args = argparse.Namespace(**args_dict)\n",
    "print(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "at783kr7KvS4"
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    early_stop_callback=False,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LBvpP01KvTA"
   },
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, type_path, args):\n",
    "  return EmotionDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3Tty_OHGlvR"
   },
   "outputs": [],
   "source": [
    "model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIsW9pwEG27D"
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmk4GsEMHTfZ"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwdWdHG0RP5J"
   },
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dq7cCiOPRQzs"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKsHzqGMRQzz"
   },
   "outputs": [],
   "source": [
    "dataset = EmotionDataset(tokenizer, 'emotion_data', 'test', 512)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QK7s7IpERQz5"
   },
   "outputs": [],
   "source": [
    "it = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_79Jk36RQz-"
   },
   "outputs": [],
   "source": [
    "batch = next(it)\n",
    "batch[\"source_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQZKyEaVRQ0B"
   },
   "outputs": [],
   "source": [
    "outs = model.model.generate(input_ids=batch['source_ids'].cuda(), \n",
    "                              attention_mask=batch['source_mask'].cuda(), \n",
    "                              max_length=2)\n",
    "\n",
    "dec = [tokenizer.decode(ids) for ids in outs]\n",
    "\n",
    "texts = [tokenizer.decode(ids) for ids in batch['source_ids']]\n",
    "targets = [tokenizer.decode(ids) for ids in batch['target_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAjhiBcrRQ0E"
   },
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    c = texts[i]\n",
    "    lines = textwrap.wrap(\"text:\\n%s\\n\" % c, width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual sentiment: %s\" % targets[i])\n",
    "    print(\"predicted sentiment: %s\" % dec[i])\n",
    "    print(\"=====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iq8M8nbTSJlE"
   },
   "source": [
    "#### Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-oIXmoCR6kl"
   },
   "outputs": [],
   "source": [
    "dataset = EmotionDataset(tokenizer, 'emotion_data', 'test', 512)\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=4)\n",
    "model.model.eval()\n",
    "outputs = []\n",
    "targets = []\n",
    "for batch in tqdm(loader):\n",
    "  outs = model.model.generate(input_ids=batch['source_ids'].cuda(), \n",
    "                              attention_mask=batch['source_mask'].cuda(), \n",
    "                              max_length=2)\n",
    "\n",
    "  dec = [tokenizer.decode(ids) for ids in outs]\n",
    "  target = [tokenizer.decode(ids) for ids in batch[\"target_ids\"]]\n",
    "  \n",
    "  outputs.extend(dec)\n",
    "  targets.extend(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9CYCGM6SRzb"
   },
   "outputs": [],
   "source": [
    "for i, out in enumerate(outputs):\n",
    "  if out not in emotions:\n",
    "    print(i, 'detected invalid prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iE0WX_GbSRzq"
   },
   "outputs": [],
   "source": [
    "metrics.accuracy_score(targets, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWkOZ7BASRz5"
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(targets, outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6p9MGb6lWL5"
   },
   "source": [
    "Now lets plot  the confusion matrix and see for which classes our model is getting confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RtgfuzucFeN"
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ioVvq5rcHZE"
   },
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(targets, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rM5XS09SSdm"
   },
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(cm, index = [\"anger\", \"fear\", \"joy\", \"love\", \"sadness\", \"surprise\"], columns = [\"anger\", \"fear\", \"joy\", \"love\", \"sadness\", \"surprise\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True, cmap='Purples', fmt='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKh_bJxtlhkW"
   },
   "source": [
    "From the above plot we can see that the most confused classes are 'joy' and 'love' which seems obivous as these two emotions are really close. We can say the same thing 'surprise' and 'anger' as well. So our model is doing pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16TiclmeX1xE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZ-YLmJyg64T"
   },
   "source": [
    "## SWAG\n",
    "\n",
    "Now lets try a more challenging task and see how it performs.\n",
    "\n",
    "SWAG is a natural language inference and commonsense reasoning task proposed in this [paper](https://arxiv.org/pdf/1808.05326.pdf).\n",
    "\n",
    "The basic task is that  a model is\n",
    "given a context **c = (s, n)**: a complete sentence\n",
    "**s** and a noun phrase **n** that begins a second sentence, as well as a list of possible verb phrase sentence endings **V**. The model must then\n",
    "select the most appropriate verb phrase **v** in **V**. For example\n",
    "\n",
    "On stage, a woman takes a seat at the piano. She\n",
    "\n",
    "a) sits on a bench as her sister plays with the doll.\n",
    "\n",
    "b) smiles with someone as the music plays.\n",
    "\n",
    "c) is in the crowd, watching the dancers.\n",
    "\n",
    "**d) nervously sets her fingers on the keys.**\n",
    "\n",
    "The correct answer is bolded. Given the above example the model should select **nervously sets her fingers on the keys** as the most appropriate verb phrase\n",
    "\n",
    "To frame this task in text-2-text setting the example is processed as below.\n",
    "\n",
    "context: context_text options: 1: option_1 2: option_2 3: option_3 4: option_4\n",
    "\n",
    "and if the actual label is 1 then the model is asked to predict the text '1'. Here's how the above example will be processed\n",
    "\n",
    "**Input**\n",
    "\n",
    "context: On stage, a woman takes a seat at the piano. She  options: 1: sits on a bench as her sister plays with the doll. 2: smiles with someone as the music plays. 3: is in the crowd, watching the dancers. 4: nervously sets her fingers on the keys.\n",
    "\n",
    "**Target**\n",
    "\n",
    "4\n",
    "\n",
    "This is just one possible way to process these examples, there are various other ways we can formulate this problem in text-2-text setting but that's for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOxk-ZoJmamm"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeHfgOhThLPj"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from enum import Enum\n",
    "from typing import List, Optional\n",
    "from transformers import PreTrainedTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DulV7U5hik7"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/rowanz/swagaf/master/data/train.csv\n",
    "!wget https://raw.githubusercontent.com/rowanz/swagaf/master/data/val.csv\n",
    "\n",
    "!mkdir swag_data\n",
    "!mv *.csv swag_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tllm6irZg8IO"
   },
   "outputs": [],
   "source": [
    "# below code is adapted from https://github.com/huggingface/transformers/blob/master/examples/multiple-choice/utils_multiple_choice.py\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class InputExample:\n",
    "    \"\"\"\n",
    "    A single training/test example for multiple choice\n",
    "    Args:\n",
    "        example_id: Unique id for the example.\n",
    "        question: string. The untokenized text of the second sequence (question).\n",
    "        contexts: list of str. The untokenized text of the first sequence (context of corresponding question).\n",
    "        endings: list of str. multiple choice's options. Its length must be equal to contexts' length.\n",
    "        label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "\n",
    "    example_id: str\n",
    "    context: str\n",
    "    endings: List[str]\n",
    "    label: Optional[str]\n",
    "\n",
    "class Split(Enum):\n",
    "    train = \"train\"\n",
    "    dev = \"dev\"\n",
    "    test = \"test\"\n",
    "\n",
    "class DataProcessor:\n",
    "    \"\"\"Base class for data converters for multiple choice data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class SwagProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the SWAG data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} train\".format(data_dir))\n",
    "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n",
    "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"val.csv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n",
    "        raise ValueError(\n",
    "            \"For swag testing, the input file does not contain a label column. It can not be tested in current code\"\n",
    "            \"setting!\"\n",
    "        )\n",
    "        return self._create_examples(self._read_csv(os.path.join(data_dir, \"test.csv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\", \"2\", \"3\"]\n",
    "\n",
    "    def _read_csv(self, input_file):\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return list(csv.reader(f))\n",
    "\n",
    "    def _create_examples(self, lines: List[List[str]], type: str):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        if type == \"train\" and lines[0][-1] != \"label\":\n",
    "            raise ValueError(\"For training, the input file must contain a label column.\")\n",
    "\n",
    "        examples = [\n",
    "            InputExample(\n",
    "                example_id=line[2],\n",
    "                # common beginning of each\n",
    "                # choice is stored in \"sent2\".\n",
    "                context=line[3],\n",
    "                endings=[line[7], line[8], line[9], line[10]],\n",
    "                label=line[11],\n",
    "            )\n",
    "            for line in lines[1:]  # we skip the line with the column names\n",
    "        ]\n",
    "\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OXxGvqZjC9L"
   },
   "outputs": [],
   "source": [
    "class SwagDataset(Dataset):\n",
    "  def __init__(self, tokenizer, data_dir, type_path,  max_len=512):\n",
    "    self.data_dir = data_dir\n",
    "    self.type_path = type_path\n",
    "    self.max_len = max_len\n",
    "    self.tokenizer = tokenizer\n",
    "    self.inputs = []\n",
    "    self.targets = []\n",
    "\n",
    "    self.proc = SwagProcessor()\n",
    "\n",
    "    self._build()\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "    target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "    src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "    target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "    return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "  \n",
    "  def _build(self):\n",
    "    if self.type_path == 'train':\n",
    "      examples = self.proc.get_train_examples(self.data_dir)\n",
    "    else:\n",
    "      examples = self.proc.get_dev_examples(self.data_dir)\n",
    "    \n",
    "    for example in examples:\n",
    "      self._create_features(example)\n",
    "  \n",
    "  def _create_features(self, example):\n",
    "    input_ = example.context\n",
    "    options = ['%s: %s' % (i, option) for i, option in zip('1234', example.endings)]\n",
    "    options = \" \".join(options)\n",
    "    input_ = \"context: %s  options: %s </s>\" % (input_, options)\n",
    "    target = \"%s </s>\" % str(int(example.label) + 1)\n",
    "\n",
    "    # tokenize inputs\n",
    "    tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "        [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    # tokenize targets\n",
    "    tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "        [target], max_length=2, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    self.inputs.append(tokenized_inputs)\n",
    "    self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKqFMTku3sDC"
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIUiU7zSpbb3"
   },
   "outputs": [],
   "source": [
    "dataset = SwagDataset(tokenizer, data_dir='swag_data', type_path='val')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxXGbCzB37HG"
   },
   "outputs": [],
   "source": [
    "data = dataset[69]\n",
    "print(tokenizer.decode(data['source_ids']))\n",
    "print(tokenizer.decode(data['target_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVfmE4O3Ku7H"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDPxWUY86llx"
   },
   "outputs": [],
   "source": [
    "!mkdir -p t5_swag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrWtMjcj6lmA"
   },
   "outputs": [],
   "source": [
    "args_dict.update({'data_dir': 'swag_data', 'output_dir': 't5_swag', 'num_train_epochs': 3})\n",
    "args = argparse.Namespace(**args_dict)\n",
    "print(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ojz3THj6lmK"
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    early_stop_callback=False,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kk0x0Nql6lmQ"
   },
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, type_path, args):\n",
    "  return SwagDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDFGzzpQ6lmU"
   },
   "outputs": [],
   "source": [
    "model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sQVILFo63Eb"
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STkqK5nC64YP"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1ZB_6SK7V-3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgNV3TMzqSvj"
   },
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFFOwfXyqc4_"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsYCq3Lwqc5Y"
   },
   "outputs": [],
   "source": [
    "dataset =  SwagDataset(tokenizer, data_dir='swag_data', type_path='val')\n",
    "loader = DataLoader(dataset, batch_size=32, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHwMBQNjqc5h"
   },
   "outputs": [],
   "source": [
    "model.model.eval()\n",
    "outputs = []\n",
    "targets = []\n",
    "for batch in tqdm(loader):\n",
    "  outs = model.model.generate(input_ids=batch['source_ids'].cuda(), \n",
    "                              attention_mask=batch['source_mask'].cuda(), \n",
    "                              max_length=2)\n",
    "\n",
    "  dec = [tokenizer.decode(ids) for ids in outs]\n",
    "  target = [tokenizer.decode(ids) for ids in batch[\"target_ids\"]]\n",
    "  \n",
    "  outputs.extend(dec)\n",
    "  targets.extend(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbTValmYq15r"
   },
   "outputs": [],
   "source": [
    "for i, out in enumerate(outputs):\n",
    "  if out not in \"1234\":\n",
    "    print(i, 'detected invalid prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jN35n2pas-pF"
   },
   "outputs": [],
   "source": [
    "metrics.accuracy_score(targets, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_WaMutznvGb"
   },
   "source": [
    "This is great! We have achieved almost 74% accuracy with this simple formulation. This is great becuase with BERT like models to make a prediction on single example the model needs to do 4 forward passes, one for each possible endings and then the logits are concatenated together for all 4 passes and then passed through final softmax layer to produce 4 probabilities. This approach needs only a single pass for one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFgOHlW_tHPd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Dhqigmiw2hVh",
    "0B4IhzEgO21B",
    "cANrUEXhO8QY",
    "DEWi6c-pGZV9",
    "GwdWdHG0RP5J",
    "iq8M8nbTSJlE",
    "AgNV3TMzqSvj"
   ],
   "machine_shape": "hm",
   "name": "Copy of t5_fine-tuning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06267db87bf142058775dddf3f589299": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46a2e458d20f403587bee733a30c8f9c",
      "placeholder": "​",
      "style": "IPY_MODEL_5b6828f6efe847a7a90ec48b03f76a71",
      "value": " 792k/792k [00:09&lt;00:00, 80.3kB/s]"
     }
    },
    "124075f5a54c46aa8d488478c5155c3a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "20d649a0fedd445f931f31e637c6f460": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24957342340b48ed9b6c8dc3dfe22a7e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26da2def11c04013b99ba9fc06f0f97e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d263a629b3f408f8a72e9dcec5159cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Validation sanity check: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ee2294de22c4cc380d0000e6e99dae5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d97b3deb4aaa44739cc228ae7bcdbc1b",
      "value": 1
     }
    },
    "46a2e458d20f403587bee733a30c8f9c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56f9052eb67147e7b2152503e2d67807": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b6828f6efe847a7a90ec48b03f76a71": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "764fc71349e04ca2bc56885684bc406b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b32951e9fbc64680b03d61235ee13bda",
      "max": 1199,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d76ae256aaf84e37a03d8bd423d49dd5",
      "value": 1199
     }
    },
    "7ee2294de22c4cc380d0000e6e99dae5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "863720f4f8834cda8bd0de0197663e63": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "89c7a8ac42c24078a65e7afcfea5c2fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_be778f6359224ff9b9d1d99b0eb68ae0",
       "IPY_MODEL_f276ed431c60493e8e166bc9c711c852"
      ],
      "layout": "IPY_MODEL_26da2def11c04013b99ba9fc06f0f97e"
     }
    },
    "9157b33faf284f3296e20450cc5e6d17": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "947d04f0c06c4464acfec292e9e78ba0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cdae1eac7cc4aa1a31137941fabda03": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e868445191a4546b914e370e3a3653a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa844d6a281f4c18be3be48ec30a2091": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_947d04f0c06c4464acfec292e9e78ba0",
      "max": 791656,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_863720f4f8834cda8bd0de0197663e63",
      "value": 791656
     }
    },
    "b32951e9fbc64680b03d61235ee13bda": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b97fa18acaf5454a86540f14b52976be": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be778f6359224ff9b9d1d99b0eb68ae0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56f9052eb67147e7b2152503e2d67807",
      "max": 891691430,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed90ad03272845f2bd803c59272d8b4c",
      "value": 891691430
     }
    },
    "c66f16e3f218442386d3c6137aea6824": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa844d6a281f4c18be3be48ec30a2091",
       "IPY_MODEL_06267db87bf142058775dddf3f589299"
      ],
      "layout": "IPY_MODEL_9e868445191a4546b914e370e3a3653a"
     }
    },
    "cada440810c64395a0af0744ddbfcfdf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc5547e7bfe14eda9dc9fd58d907f7a7",
      "placeholder": "​",
      "style": "IPY_MODEL_20d649a0fedd445f931f31e637c6f460",
      "value": " 5/5 [00:03&lt;00:00,  1.34it/s]"
     }
    },
    "d76ae256aaf84e37a03d8bd423d49dd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d97b3deb4aaa44739cc228ae7bcdbc1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d99dbf5460e24a21a070df9b4f80ce27": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24957342340b48ed9b6c8dc3dfe22a7e",
      "placeholder": "​",
      "style": "IPY_MODEL_9cdae1eac7cc4aa1a31137941fabda03",
      "value": " 1.20k/1.20k [00:00&lt;00:00, 8.76kB/s]"
     }
    },
    "dbbbcab8656d4a189f8c647bf4298c9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_764fc71349e04ca2bc56885684bc406b",
       "IPY_MODEL_d99dbf5460e24a21a070df9b4f80ce27"
      ],
      "layout": "IPY_MODEL_9157b33faf284f3296e20450cc5e6d17"
     }
    },
    "dc5547e7bfe14eda9dc9fd58d907f7a7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed90ad03272845f2bd803c59272d8b4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ed97db83a8e649c0a495dc319d3d6fad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f276ed431c60493e8e166bc9c711c852": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b97fa18acaf5454a86540f14b52976be",
      "placeholder": "​",
      "style": "IPY_MODEL_ed97db83a8e649c0a495dc319d3d6fad",
      "value": " 892M/892M [00:24&lt;00:00, 35.8MB/s]"
     }
    },
    "f40fb3182787435d81fa9f910ba63a83": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3d263a629b3f408f8a72e9dcec5159cd",
       "IPY_MODEL_cada440810c64395a0af0744ddbfcfdf"
      ],
      "layout": "IPY_MODEL_124075f5a54c46aa8d488478c5155c3a"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
